import openai as _openai
import logging
from typing import Dict, Any, Optional, List
import asyncio
import json
import time
from datetime import datetime

logger = logging.getLogger(__name__)

class OpenAIAgent:
    def __init__(self, openai_api_key: str = None):
        """
        Initialize OpenAI Agent with GPT models
        Enhanced setup similar to Gemini with smart model testing
        """
        self.available_models = {}
        self.current_model_name = None
        self.client = None
        self.last_error: Optional[str] = None
        self.openai_api_key = openai_api_key
        self.offline_mode = True  # Default to offline until proven otherwise
        self.model_capabilities = {
            'gpt-4o': {
                'strengths': ['analysis', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'multilingual'],
                'speed': 'medium',
                'cost': 'high',
                'tier': 'premium'
            },
            'gpt-4o-mini': {
                'strengths': ['analysis', 'reasoning', 'fast_response', 'cost_effective'],
                'speed': 'fast',
                'cost': 'low',
                'tier': 'standard'
            },
            'gpt-4-turbo': {
                'strengths': ['analysis', 'reasoning', 'long_context', 'technical_analysis'],
                'speed': 'medium',
                'cost': 'high',
                'tier': 'premium'
            },
            'gpt-4': {
                'strengths': ['analysis', 'reasoning', 'financial_advice'],
                'speed': 'slow',
                'cost': 'very_high',
                'tier': 'premium'
            },
            'gpt-3.5-turbo': {
                'strengths': ['general_purpose', 'fast_response', 'cost_effective'],
                'speed': 'very_fast',
                'cost': 'very_low',
                'tier': 'basic'
            },
            'gpt-4-turbo-preview': {
                'strengths': ['analysis', 'reasoning', 'preview_features'],
                'speed': 'medium',
                'cost': 'high',
                'tier': 'premium'
            }
        }

        if self.openai_api_key:
            try:
                self.client = _openai.OpenAI(api_key=self.openai_api_key)

                # Try different GPT models in order of preference (similar to Gemini)
                model_names = [
                    'gpt-4o',                # Latest GPT-4 Omni (best quality)
                    'gpt-4o-mini',           # Cost-effective GPT-4 (RECOMMENDED)
                    'gpt-4-turbo',           # GPT-4 Turbo
                    'gpt-4-turbo-preview',   # Preview version
                    'gpt-4',                 # Standard GPT-4 (expensive)
                    'gpt-3.5-turbo',         # Fallback option (cheap)
                    'gpt-3.5-turbo-16k'      # Extended context fallback
                ]

                model_initialized = False
                for model_name in model_names:
                    try:
                        # Skip test for expensive models to avoid quota usage (like Gemini Pro)
                        if self._is_expensive_model(model_name):
                            # Just initialize without testing to avoid costs
                            self.available_models['openai'] = model_name
                            self.current_model_name = model_name
                            self.offline_mode = False
                            logger.info(f"âœ… OpenAI initialized with model: {model_name} (no test - premium model)")
                            model_initialized = True
                            break
                        else:
                            # Test cheaper models with minimal request
                            test_response = self.client.chat.completions.create(
                                model=model_name,
                                messages=[{"role": "user", "content": "Hi"}],
                                max_tokens=5,
                                timeout=10
                            )

                            if test_response and test_response.choices:
                                self.available_models['openai'] = model_name
                                self.current_model_name = model_name
                                self.offline_mode = False
                                logger.info(f"âœ… OpenAI initialized with model: {model_name}")
                                model_initialized = True
                                break

                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'quota' in error_msg or 'rate limit' in error_msg or '429' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} quota/rate limit exceeded, trying next...")
                        elif 'insufficient_quota' in error_msg or 'billing' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} billing issue, trying next...")
                        elif 'invalid' in error_msg and 'api' in error_msg:
                            logger.warning(f"âš ï¸ Invalid API key for {model_name}")
                        elif 'model not found' in error_msg or 'does not exist' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} not available, trying next...")
                        else:
                            logger.warning(f"âš ï¸ Model {model_name} error: {e}")
                        continue

                if not model_initialized:
                    msg = "No OpenAI models available, will use offline mode"
                    logger.warning(f"âš ï¸ {msg}")
                    self.last_error = f"âŒ LLM cÃ³ lá»—i: {msg}"
                    self.available_models = {}

            except Exception as e:
                err = str(e)
                logger.error(f"âŒ Failed to initialize OpenAI: {err}")
                self.last_error = f"âŒ LLM cÃ³ lá»—i: {err}"
                self.available_models = {}

        # Allow initialization without models for offline mode (like Gemini)
        if not self.available_models:
            logger.warning("âš ï¸ No OpenAI models available, system will run in offline mode")
            self.offline_mode = True
        else:
            self.offline_mode = False

    def _is_expensive_model(self, model_name: str) -> bool:
        """Check if model is expensive and should skip testing"""
        expensive_keywords = ['gpt-4o', 'gpt-4-turbo']
        return any(keyword in model_name for keyword in expensive_keywords)
    
    def test_connection(self):
        """Test OpenAI API connection"""
        results = {}
        
        if 'openai' in self.available_models and self.client:
            try:
                response = self.client.chat.completions.create(
                    model=self.current_model_name,
                    messages=[{"role": "user", "content": "Test"}],
                    max_tokens=5
                )
                if response and response.choices:
                    results['openai'] = True
                    logger.info("âœ… OpenAI connection test passed")
                else:
                    results['openai'] = False
                    msg = "OpenAI returned empty response"
                    logger.error(f"âŒ {msg}")
                    self.last_error = f"âŒ LLM cÃ³ lá»—i: {msg}"
            except Exception as e:
                results['openai'] = False
                err = str(e)
                logger.error(f"âŒ OpenAI connection test failed: {err}")
                self.last_error = f"âŒ LLM cÃ³ lá»—i: {err}"
        
        return results
    
    def generate_with_model(self, prompt: str, max_tokens: int = 2000) -> str:
        """
        Generate response using OpenAI GPT model
        """
        try:
            if 'openai' in self.available_models and self.client:
                response = self.client.chat.completions.create(
                    model=self.current_model_name,
                    messages=[
                        {"role": "system", "content": "Báº¡n lÃ  chuyÃªn gia phÃ¢n tÃ­ch tÃ i chÃ­nh chuyÃªn nghiá»‡p, tráº£ lá»i báº±ng tiáº¿ng Viá»‡t."},
                        {"role": "user", "content": prompt}
                    ],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                return response.choices[0].message.content
            else:
                raise ValueError("OpenAI model not available")
                
        except Exception as e:
            err = str(e)
            logger.error(f"Error generating with OpenAI: {err}")
            self.last_error = f"âŒ LLM cÃ³ lá»—i: {err}"
            raise

    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 2000) -> Dict[str, Any]:
        """
        Generate response with automatic fallback to offline mode
        """
        if getattr(self, 'offline_mode', True) or not self.available_models:
            logger.info("ğŸ“´ Using offline mode (no OpenAI models available)")
            return self._generate_offline_fallback(prompt, task_type)
        
        try:
            response = self.generate_with_model(prompt, max_tokens)
            return {
                'response': response,
                'model_used': f'openai_{self.current_model_name}',
                'success': True
            }
        except Exception as e:
            err = str(e)
            logger.error(f"OpenAI model failed: {err}")
            fallback = self._generate_offline_fallback(prompt, task_type)
            fallback['response'] = f"âŒ LLM cÃ³ lá»—i: {err}.\n\n" + fallback.get('response', '')
            fallback['error'] = err
            return fallback
    
    def _generate_offline_fallback(self, prompt: str, task_type: str) -> Dict[str, Any]:
        """
        Generate offline fallback response when API fails
        """
        try:
            if 'CÃ‚U Há»I:' in prompt:
                question = prompt.split('CÃ‚U Há»I:')[1].split('MÃƒ Cá»” PHIáº¾U:')[0].strip()
            else:
                question = prompt[:200] + '...' if len(prompt) > 200 else prompt
            
            if task_type == 'financial_advice':
                response = self._generate_financial_advice_fallback(question)
            elif task_type == 'general_query':
                response = self._generate_general_fallback(question)
            else:
                response = self._generate_default_fallback(question)
            
            return {
                'response': response,
                'model_used': 'openai_offline_fallback',
                'success': True,
                'quota_exceeded': True
            }
        except Exception as e:
            return {
                'response': f'OpenAI offline fallback failed: {str(e)}',
                'model_used': 'openai_offline_fallback',
                'success': False,
                'error': str(e)
            }
    
    def _generate_financial_advice_fallback(self, question: str) -> str:
        """Generate financial advice fallback"""
        return f"""
ğŸ“Š PHÃ‚N TÃCH OPENAI OFFLINE:
Do OpenAI API khÃ´ng kháº£ dá»¥ng, há»‡ thá»‘ng chuyá»ƒn sang cháº¿ Ä‘á»™ offline vá»›i phÃ¢n tÃ­ch cÆ¡ báº£n:

ğŸ’¡ NguyÃªn táº¯c Ä‘áº§u tÆ° GPT:
- Äa dáº¡ng hÃ³a danh má»¥c Ä‘á»ƒ phÃ¢n tÃ¡n rá»§i ro
- Äáº§u tÆ° dÃ i háº¡n thÆ°á»ng mang láº¡i lá»£i nhuáº­n tá»‘t hÆ¡n
- Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t
- NghiÃªn cá»©u ká»¹ trÆ°á»›c khi Ä‘áº§u tÆ° (DYOR)

ğŸ“ˆ PhÃ¢n tÃ­ch ká»¹ thuáº­t cÆ¡ báº£n:
- Theo dÃµi xu hÆ°á»›ng giÃ¡ vÃ  khá»‘i lÆ°á»£ng giao dá»‹ch
- Sá»­ dá»¥ng cÃ¡c chá»‰ bÃ¡o nhÆ° RSI, MACD, MA
- Xem xÃ©t má»©c há»— trá»£ vÃ  khÃ¡ng cá»±

âš ï¸ LÆ°u Ã½: ÄÃ¢y chá»‰ lÃ  thÃ´ng tin tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ°.
ğŸ”„ API thÆ°á»ng reset sau 24 giá» hoáº·c khi quota Ä‘Æ°á»£c gia háº¡n.
"""
    
    def _generate_general_fallback(self, question: str) -> str:
        """Generate general fallback"""
        return f"""
ğŸ¤– OPENAI OFFLINE MODE:
CÃ¢u há»i cá»§a báº¡n: {question}

Do OpenAI API táº¡m thá»i khÃ´ng kháº£ dá»¥ng, tÃ´i khÃ´ng thá»ƒ cung cáº¥p phÃ¢n tÃ­ch chi tiáº¿t.

ğŸ’¡ Gá»£i Ã½:
- Kiá»ƒm tra láº¡i API key OpenAI
- Äáº£m báº£o cÃ³ Ä‘á»§ credits trong tÃ i khoáº£n
- Thá»­ láº¡i sau vÃ i phÃºt

ğŸ”„ Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng chuyá»ƒn vá» OpenAI khi API hoáº¡t Ä‘á»™ng trá»Ÿ láº¡i.
"""
    
    def _generate_default_fallback(self, question: str) -> str:
        """Generate default fallback"""
        return f"""
ğŸ¤– OPENAI FALLBACK:
CÃ¢u há»i: {question}

Do OpenAI API khÃ´ng kháº£ dá»¥ng, há»‡ thá»‘ng sá»­ dá»¥ng pháº£n há»“i cÆ¡ báº£n.

ğŸ’¡ Äá»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t hÆ¡n:
- Kiá»ƒm tra OpenAI credits táº¡i: https://platform.openai.com/usage
- Hoáº·c sá»­ dá»¥ng Gemini API (miá»…n phÃ­) thay tháº¿
- Thá»­ láº¡i sau vÃ i phÃºt

ğŸ”„ Há»‡ thá»‘ng váº«n hoáº¡t Ä‘á»™ng vá»›i cÃ¡c tÃ­nh nÄƒng khÃ¡c.
"""
        return f"""
ğŸ¤– OPENAI SYSTEM OFFLINE:
OpenAI API hiá»‡n khÃ´ng kháº£ dá»¥ng. Vui lÃ²ng:

1. Kiá»ƒm tra káº¿t ná»‘i internet
2. XÃ¡c minh API key OpenAI
3. Kiá»ƒm tra quota/credits
4. Thá»­ láº¡i sau Ã­t phÃºt

ğŸ“ Há»— trá»£: LiÃªn há»‡ admin náº¿u váº¥n Ä‘á» kÃ©o dÃ i.
"""