import google.generativeai as genai
from openai import OpenAI
import os
import logging
from typing import Dict, Any, Optional, List
import asyncio
import json
import time
from datetime import datetime

try:
    from litellm import completion
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False
    completion = None

logger = logging.getLogger(__name__)
class UnifiedAIAgent:
    def __init__(self, gemini_api_key: str = None, openai_api_key: str = None, preferred_model: str = "auto"):
        """
        Initialize AI Agent with Gemini AI, OpenAI, and Llama
        """
        self.available_models = {}
        self.current_model_name = None
        self.preferred_model = preferred_model
        self.openai_client = None
        self.model_capabilities = {
            'gemini': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'risk_assessment'],
                'speed': 'fast',
                'cost': 'free'
            },
            'openai': {
                'strengths': ['analysis', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'english'],
                'speed': 'medium',
                'cost': 'paid'
            },
            'llama': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'local_processing'],
                'speed': 'medium',
                'cost': 'free_local'
            }
        }
        
        # Initialize AI models with user-provided API keys
        # No hardcoded or environment variables used
        
        # Initialize Llama (local)
        if LITELLM_AVAILABLE:
            try:
                # Test Llama availability
                test_response = completion(
                    model="ollama/llama3.1:8b",
                    messages=[{"role": "user", "content": "test"}],
                    temperature=0.1,
                    max_tokens=10,
                )
                if test_response and test_response.choices:
                    self.available_models['llama'] = 'ollama/llama3.1:8b'
                    logger.info("âœ… Llama 3.1:8b initialized (local)")
            except Exception as e:
                logger.warning(f"âš ï¸ Llama not available: {str(e)}")
        else:
            logger.warning("âš ï¸ litellm not installed, Llama unavailable")
        
        # Initialize OpenAI
        if openai_api_key:
            try:
                self.openai_client = OpenAI(api_key=openai_api_key)
                self.openai_api_key = openai_api_key
                
                # Try different OpenAI models
                openai_models = [
                    'gpt-4o',           # Latest GPT-4 Omni
                    'gpt-4-turbo',      # GPT-4 Turbo
                    'gpt-4',            # Standard GPT-4
                    'gpt-3.5-turbo'     # Fallback
                ]
                
                # Just set the first available model without testing to avoid API calls during init
                self.available_models['openai'] = openai_models[0]
                logger.info(f"âœ… OpenAI initialized with model: {openai_models[0]}")
                        
            except Exception as e:
                logger.error(f"âŒ Failed to initialize OpenAI: {str(e)}")
        
        # Initialize Gemini
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                
                # Try different model names (API v1beta compatible)
                model_names = [
                    'gemini-3-pro-preview',        # Flagship má»›i nháº¥t, kháº£ nÄƒng suy luáº­n cao
                    'gemini-3-flash-preview',      # Tá»‘c Ä‘á»™ cao tháº¿ há»‡ 3

                        # --- GEMINI 2.5 SERIES (TiÃªu chuáº©n - Production Ready) ---
                    'gemini-2.5-pro',              # Báº£n chuáº©n tá»‘t nháº¥t cho má»i tÃ¡c vá»¥ (thay tháº¿ 1.5 Pro)
                    'gemini-2.5-flash',            # Báº£n chuáº©n tá»‘c Ä‘á»™ cao (thay tháº¿ 1.5 Flash)
                    'gemini-2.5-flash-lite',       # Chi phÃ­ cá»±c tháº¥p, tá»‘i Æ°u cho tÃ¡c vá»¥ Ä‘Æ¡n giáº£n
                    'gemini-2.5-pro-001',          # Báº£n snapshot cá»¥ thá»ƒ (trÃ¡nh thay Ä‘á»•i ngáº§m)
                    'gemini-2.5-flash-001',        # Báº£n snapshot cá»¥ thá»ƒ

                    # --- GEMINI 2.0 SERIES (á»”n Ä‘á»‹nh cÅ©) ---
                    'gemini-2.0-flash',            # Fallback tin cáº­y
                    'gemini-2.0-flash-exp',        # Báº£n thá»­ nghiá»‡m cÅ© (cÃ³ thá»ƒ váº«n hoáº¡t Ä‘á»™ng)

                    # --- LEGACY (CÅ© - Háº¡n cháº¿ dÃ¹ng cho dá»± Ã¡n má»›i) ---
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-1.5-flash-8b',         # Báº£n siÃªu nháº¹ Ä‘á»i cÅ©

                    # --- SPECIALIZED (NhÃºng & HÃ¬nh áº£nh) ---
                    'text-embedding-005',          # Model nhÃºng vÄƒn báº£n má»›i nháº¥t (Semantic Search)
                    'imagen-3.0-generate-001',     # Táº¡o áº£nh
                    'aqa'             # Legacy fallback
                ]
                
                model_initialized = False
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Initialize without testing to avoid quota usage
                        self.available_models['gemini'] = model
                        self.gemini_api_key = gemini_api_key
                        self.current_model_name = model_name
                        logger.info(f"âœ… Gemini AI initialized with model: {model_name}")
                        model_initialized = True
                        break
                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'quota' in error_msg or '429' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} quota exceeded, trying next...")
                        elif '404' in error_msg or 'not found' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} not found, trying next...")
                        else:
                            logger.warning(f"âš ï¸ Model {model_name} error: {e}")
                        continue
                
                if not model_initialized:
                    # If no model works, still allow offline mode
                    logger.warning("âš ï¸ No Gemini models available, will use offline mode")
                    self.available_models = {}
                    
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Gemini: {str(e)}")
                # Don't set available_models if initialization failed
                self.available_models = {}
        
        # Set offline mode based on available models
        if not self.available_models:
            logger.warning("âš ï¸ No AI models available, system will run in offline mode")
            self.offline_mode = True
        else:
            self.offline_mode = False
            logger.info(f"âœ… AI models available: {list(self.available_models.keys())}")
    
    def test_connection(self):
        """Test AI API connections without using quota"""
        results = {}
        
        # Test Gemini
        if 'gemini' in self.available_models:
            try:
                # Just check if model exists, don't make API call
                if hasattr(self, 'gemini_api_key') and self.gemini_api_key:
                    results['gemini'] = True
                    logger.info("âœ… Gemini model available")
                else:
                    results['gemini'] = False
            except:
                results['gemini'] = False
        else:
            results['gemini'] = False
        
        # Test OpenAI
        if 'openai' in self.available_models:
            try:
                # Just check if client exists, don't make API call
                if hasattr(self, 'openai_client') and self.openai_client:
                    results['openai'] = True
                    logger.info("âœ… OpenAI model available")
                else:
                    results['openai'] = False
            except:
                results['openai'] = False
        else:
            results['openai'] = False
        
        # Test Llama
        if 'llama' in self.available_models:
            try:
                # Just check if litellm is available
                if LITELLM_AVAILABLE:
                    results['llama'] = True
                    logger.info("âœ… Llama model available")
                else:
                    results['llama'] = False
            except:
                results['llama'] = False
        else:
            results['llama'] = False
        
        return results
    
    def select_best_model(self, task_type: str) -> str:
        """
        Select the best available model for a specific task type based on user preference
        """
        # Respect user preference first
        if self.preferred_model == "gemini" and 'gemini' in self.available_models:
            return 'gemini'
        elif self.preferred_model == "openai" and 'openai' in self.available_models:
            return 'openai'
        elif self.preferred_model == "llama" and 'llama' in self.available_models:
            return 'llama'
        elif self.preferred_model == "auto":
            # Auto mode: prefer Gemini for Vietnamese content and free usage
            if 'gemini' in self.available_models:
                return 'gemini'
            # Fallback to OpenAI for English content
            if 'openai' in self.available_models:
                return 'openai'
            # Final fallback to Llama (local)
            if 'llama' in self.available_models:
                return 'llama'
        
        # Final fallback - use any available model (priority order)
        if 'gemini' in self.available_models:
            return 'gemini'
        if 'openai' in self.available_models:
            return 'openai'
        if 'llama' in self.available_models:
            return 'llama'
        
        raise ValueError("No AI models available")
    
    def generate_with_model(self, prompt: str, model_name: str, max_tokens: int = 2000) -> str:
        """
        Generate response using specified AI model
        """
        try:
            if model_name == 'gemini' and 'gemini' in self.available_models:
                response = self.available_models['gemini'].generate_content(prompt)
                return response.text
            
            elif model_name == 'openai' and 'openai' in self.available_models:
                if not hasattr(self, 'openai_client') or not self.openai_client:
                    raise ValueError("OpenAI client not initialized")
                openai_model = self.available_models['openai']
                response = self.openai_client.chat.completions.create(
                    model=openai_model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                return response.choices[0].message.content
            
            elif model_name == 'llama' and 'llama' in self.available_models:
                if not LITELLM_AVAILABLE:
                    raise ValueError("litellm not available for Llama")
                
                response = completion(
                    model="ollama/llama3.1:8b",
                    messages=[
                        {
                            "role": "system",
                            "content": (
                                "Báº¡n lÃ  trá»£ lÃ½ tÃ i chÃ­nh chuyÃªn nghiá»‡p. "
                                "Tráº£ lá»i ngáº¯n gá»n, thá»±c táº¿, báº±ng tiáº¿ng Viá»‡t. "
                                "KhÃ´ng suy Ä‘oÃ¡n sá»‘ liá»‡u chÃ­nh xÃ¡c."
                            )
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.2,
                    max_tokens=min(max_tokens, 400),
                )
                return response.choices[0].message.content

            else:
                raise ValueError(f"Model {model_name} not available.")
                
        except Exception as e:
            logger.error(f"Error generating with {model_name}: {str(e)}")
            raise

    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 2000) -> Dict[str, Any]:
        """
        Generate response with automatic fallback respecting user preference
        """
        # Check if we're already in offline mode
        if getattr(self, 'offline_mode', True) or not self.available_models:
            logger.info("ðŸ“´ Using offline mode (no AI models available)")
            return self._generate_offline_fallback(prompt, task_type)
        
        try:
            # Use preferred model first
            preferred_model = self.select_best_model(task_type)
            response = self.generate_with_model(prompt, preferred_model, max_tokens)
            return {
                'response': response,
                'model_used': preferred_model,
                'success': True
            }
        except Exception as e:
            logger.error(f"Primary model {preferred_model} failed: {str(e)}")
            # Try fallback model
            try:
                fallback_models = [m for m in self.available_models.keys() if m != preferred_model]
                if fallback_models:
                    fallback_model = fallback_models[0]
                    response = self.generate_with_model(prompt, fallback_model, max_tokens)
                    return {
                        'response': response,
                        'model_used': fallback_model,
                        'success': True
                    }
            except Exception as e2:
                logger.error(f"Fallback model failed: {str(e2)}")
            
            # Use offline fallback when all AI models fail
            return self._generate_offline_fallback(prompt, task_type)
    
    def _generate_offline_fallback(self, prompt: str, task_type: str) -> Dict[str, Any]:
        """
        Generate offline fallback response when API quota is exhausted
        """
        try:
            # Extract key information from prompt
            if 'CÃ‚U Há»ŽI:' in prompt:
                question = prompt.split('CÃ‚U Há»ŽI:')[1].split('MÃƒ Cá»” PHIáº¾U:')[0].strip()
            else:
                question = prompt[:200] + '...' if len(prompt) > 200 else prompt
            
            # Generate contextual offline response based on task type
            if task_type == 'financial_advice':
                response = self._generate_financial_advice_fallback(question)
            elif task_type == 'general_query':
                response = self._generate_general_fallback(question)
            else:
                response = self._generate_default_fallback(question)
            
            return {
                'response': response,
                'model_used': 'offline_fallback',
                'success': True,
                'quota_exceeded': True
            }
        except Exception as e:
            return {
                'response': f'Offline fallback failed: {str(e)}',
                'model_used': 'offline_fallback',
                'success': False,
                'error': str(e)
            }
    
    def _generate_financial_advice_fallback(self, question: str) -> str:
        """
        Generate financial advice fallback when API quota exceeded
        """
        return f"""
PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
Do Gemini API Ä‘Ã£ háº¿t quota, há»‡ thá»‘ng chuyá»ƒn sang cháº¿ Ä‘á»™ offline. ÄÃ¢y lÃ  phÃ¢n tÃ­ch chuyÃªn sÃ¢u dá»±a trÃªn nguyÃªn táº¯c Ä‘áº§u tÆ° thá»±c tiá»…n vÃ  kinh nghiá»‡m thá»‹ trÆ°á»ng:

ðŸ“Š **PhÃ¢n tÃ­ch ká»¹ thuáº­t:**
- Xem xÃ©t xu hÆ°á»›ng giÃ¡ 20-50 phiÃªn gáº§n nháº¥t, xÃ¡c Ä‘á»‹nh vÃ¹ng há»— trá»£/khÃ¡ng cá»±, khá»‘i lÆ°á»£ng giao dá»‹ch, chá»‰ bÃ¡o RSI/MACD.
- ÄÃ¡nh giÃ¡ dÃ²ng tiá»n, má»©c Ä‘á»™ biáº¿n Ä‘á»™ng, cÃ¡c tÃ­n hiá»‡u Ä‘áº£o chiá»u.

ðŸ’° **PhÃ¢n tÃ­ch cÆ¡ báº£n:**
- Äá»c bÃ¡o cÃ¡o tÃ i chÃ­nh, chÃº Ã½ doanh thu, lá»£i nhuáº­n, biÃªn lá»£i nhuáº­n, dÃ²ng tiá»n hoáº¡t Ä‘á»™ng.
- So sÃ¡nh P/E, P/B vá»›i trung bÃ¬nh ngÃ nh, xem xÃ©t tÄƒng trÆ°á»Ÿng EPS, ROE, ROA.
- ÄÃ¡nh giÃ¡ ban lÃ£nh Ä‘áº¡o, chiáº¿n lÆ°á»£c phÃ¡t triá»ƒn, vá»‹ tháº¿ cáº¡nh tranh.

ðŸ“° **Tin tá»©c & sá»± kiá»‡n:**
- Theo dÃµi cÃ¡c tin tá»©c áº£nh hÆ°á»Ÿng Ä‘áº¿n ngÃ nh, cá»• phiáº¿u, chÃ­nh sÃ¡ch vÄ© mÃ´, lÃ£i suáº¥t, tá»· giÃ¡.
- ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a cÃ¡c sá»± kiá»‡n Ä‘áº·c biá»‡t (chia cá»• tá»©c, phÃ¡t hÃ nh thÃªm, M&A).

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
- Chá»‰ Ä‘áº§u tÆ° khi hiá»ƒu rÃµ doanh nghiá»‡p, ngÃ nh vÃ  xu hÆ°á»›ng thá»‹ trÆ°á»ng.
- Äáº·t má»¥c tiÃªu lá»£i nhuáº­n, Ä‘iá»ƒm cáº¯t lá»— rÃµ rÃ ng.
- Äa dáº¡ng hÃ³a danh má»¥c, khÃ´ng dá»“n vá»‘n vÃ o má»™t mÃ£.
- LuÃ´n cáº­p nháº­t thÃ´ng tin, Ä‘iá»u chá»‰nh chiáº¿n lÆ°á»£c khi cÃ³ biáº¿n Ä‘á»™ng lá»›n.

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- Äá»c ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh quÃ½ gáº§n nháº¥t.
- Láº­p báº£ng so sÃ¡nh cÃ¡c mÃ£ cÃ¹ng ngÃ nh.
- Theo dÃµi diá»…n biáº¿n thá»‹ trÆ°á»ng hÃ ng ngÃ y.
- Äáº·t lá»‡nh stop-loss, take-profit cho tá»«ng vá»‹ tháº¿.
- Tham kháº£o Ã½ kiáº¿n chuyÃªn gia, cá»™ng Ä‘á»“ng Ä‘áº§u tÆ°.

Cáº¢NH BÃO Rá»¦I RO:
âš ï¸ **QUAN TRá»ŒNG:** ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline cÆ¡ báº£n do háº¿t quota API. 
KhÃ´ng nÃªn dá»±a vÃ o Ä‘Ã¢y Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ° quan trá»ng.
HÃ£y Ä‘á»£i API reset hoáº·c tham kháº£o chuyÃªn gia tÃ i chÃ­nh.
"""
    
    def _generate_general_fallback(self, question: str) -> str:
        """
        Generate general query fallback when API quota exceeded
        """
        return f"""
ðŸ“ˆ **PHÃ‚N TÃCH OFFLINE:**

Do Gemini API Ä‘Ã£ háº¿t quota, tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t cÃ¢u há»i cá»§a báº¡n lÃºc nÃ y.

**CÃ¢u há»i cá»§a báº¡n:** {question}

ðŸ’¡ **Gá»£i Ã½ Ä‘áº§u tÆ° thá»±c tiá»…n:**
- NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh, so sÃ¡nh vá»›i cÃ¡c doanh nghiá»‡p cÃ¹ng ngÃ nh.
- Äa dáº¡ng hÃ³a danh má»¥c Ä‘á»ƒ giáº£m rá»§i ro, khÃ´ng Ä‘áº§u tÆ° quÃ¡ 20% vá»‘n vÃ o má»™t mÃ£.
- Äáº·t má»¥c tiÃªu lá»£i nhuáº­n, Ä‘iá»ƒm cáº¯t lá»— rÃµ rÃ ng cho tá»«ng vá»‹ tháº¿.
- Theo dÃµi tin tá»©c, chÃ­nh sÃ¡ch vÄ© mÃ´, cÃ¡c yáº¿u tá»‘ áº£nh hÆ°á»Ÿng Ä‘áº¿n thá»‹ trÆ°á»ng.
- Tham kháº£o Ã½ kiáº¿n chuyÃªn gia, cá»™ng Ä‘á»“ng Ä‘áº§u tÆ° uy tÃ­n.
- LuÃ´n kiá»ƒm tra láº¡i chiáº¿n lÆ°á»£c khi thá»‹ trÆ°á»ng biáº¿n Ä‘á»™ng máº¡nh.

âš ï¸ **LÆ¯U Ã:** Äá»ƒ nháº­n Ä‘Æ°á»£c phÃ¢n tÃ­ch chi tiáº¿t vÃ  cÃ¡ nhÃ¢n hÃ³a, 
vui lÃ²ng thá»­ láº¡i sau khi API quota Ä‘Æ°á»£c reset (thÆ°á»ng lÃ  24h).
"""
    
    def _generate_default_fallback(self, question: str) -> str:
        """
        Generate default fallback response
        """
        return f"""
ðŸ¤– **Há»† THá»NG OFFLINE:**

Xin lá»—i, Gemini API Ä‘Ã£ háº¿t quota nÃªn tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t lÃºc nÃ y.

**CÃ¢u há»i:** {question}

**Khuyáº¿n nghá»‹ thá»±c táº¿:**
- Thá»­ láº¡i sau vÃ i giá» khi quota reset.
- Äá»c bÃ¡o cÃ¡o tÃ i chÃ­nh, phÃ¢n tÃ­ch ká»¹ thuáº­t cÆ¡ báº£n.
- Tham kháº£o cÃ¡c nguá»“n thÃ´ng tin tÃ i chÃ­nh uy tÃ­n, cá»™ng Ä‘á»“ng Ä‘áº§u tÆ°.
- Láº­p káº¿ hoáº¡ch Ä‘áº§u tÆ° rÃµ rÃ ng, kiá»ƒm soÃ¡t rá»§i ro.
- LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh náº¿u cáº§n tÆ° váº¥n gáº¥p.

â° **Quota thÆ°á»ng reset sau 24 giá»**
"""
    
    def generate_enhanced_advice(self, context: dict):
        """Generate enhanced advice with comprehensive system data"""
        query = context.get('query', '')
        symbol = context.get('symbol', '')
        system_data = context.get('system_data', {})
        query_type = context.get('query_type', 'general_inquiry')
        
        # Build enhanced context with all system data
        enhanced_context = f"""
Báº¡n lÃ  chuyÃªn gia tÃ i chÃ­nh AI hÃ ng Ä‘áº§u vá»›i kháº£ nÄƒng phÃ¢n tÃ­ch toÃ n diá»‡n há»‡ thá»‘ng trading.

CÃ‚U Há»ŽI: {query}
MÃƒ Cá»” PHIáº¾U: {symbol if symbol else 'KhÃ´ng cÃ³'}
LOáº I TRUY Váº¤N: {query_type}

Dá»® LIá»†U Há»† THá»NG TOÃ€N DIá»†N:
{self._format_comprehensive_data(system_data)}

YÃŠU Cáº¦U PHÃ‚N TÃCH:
1. ðŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U: Sá»­ dá»¥ng táº¥t cáº£ dá»¯ liá»‡u cÃ³ sáºµn tá»« há»‡ thá»‘ng, phÃ¢n tÃ­ch sÃ¢u vá» xu hÆ°á»›ng, chá»‰ sá»‘ tÃ i chÃ­nh, dÃ²ng tiá»n, tin tá»©c, dá»± Ä‘oÃ¡n giÃ¡, rá»§i ro.
2. ðŸŽ¯ PHÃ‚N TÃCH THEO LOáº I TRUY Váº¤N: Táº­p trung vÃ o {query_type}, Ä‘Æ°a ra nháº­n Ä‘á»‹nh thá»±c tiá»…n, so sÃ¡nh vá»›i cÃ¡c mÃ£ cÃ¹ng ngÃ nh, Ä‘Ã¡nh giÃ¡ triá»ƒn vá»ng.
3. ðŸ’¡ KHUYáº¾N NGHá»Š Cá»¤ THá»‚: Äá» xuáº¥t hÃ nh Ä‘á»™ng cá»¥ thá»ƒ, chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, Ä‘iá»ƒm mua/bÃ¡n, quáº£n trá»‹ rá»§i ro, phÃ¹ há»£p vá»›i tá»«ng loáº¡i nhÃ  Ä‘áº§u tÆ° (ngáº¯n háº¡n, dÃ i háº¡n).
4. âš ï¸ Rá»¦I RO & LÆ¯U Ã: ÄÃ¡nh giÃ¡ toÃ n diá»‡n cÃ¡c rá»§i ro, khuyáº¿n nghá»‹ kiá»ƒm soÃ¡t vá»‘n, Ä‘a dáº¡ng hÃ³a danh má»¥c, cáº£nh bÃ¡o cÃ¡c yáº¿u tá»‘ báº¥t thÆ°á»ng.

TRáº¢ Lá»œI THEO FORMAT:
PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
[Sá»­ dá»¥ng dá»¯ liá»‡u cá»¥ thá»ƒ tá»« há»‡ thá»‘ng, phÃ¢n tÃ­ch chi tiáº¿t tá»«ng yáº¿u tá»‘]

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
[Káº¿t luáº­n dá»±a trÃªn phÃ¢n tÃ­ch dá»¯ liá»‡u, Ä‘á» xuáº¥t hÃ nh Ä‘á»™ng thá»±c táº¿]

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- [Danh sÃ¡ch hÃ nh Ä‘á»™ng cá»¥ thá»ƒ, cÃ³ thá»ƒ Ã¡p dá»¥ng ngay]

Cáº¢NH BÃO Rá»¦I RO:
[Rá»§i ro dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿, khuyáº¿n nghá»‹ kiá»ƒm soÃ¡t]
"""
        
        try:
            result = self.generate_with_fallback(enhanced_context, 'financial_advice', max_tokens=3000)
            
            if result['success']:
                parsed_response = self._parse_response(result['response'])
                parsed_response['expert_advice'] += f"\n\nðŸ¤– **AI Model:** {result['model_used']}"
                return parsed_response
            else:
                return self._generate_enhanced_offline_response(query, symbol, system_data, query_type)
                
        except Exception as e:
            logger.error(f"Enhanced advice generation failed: {e}")
            return self._generate_enhanced_offline_response(query, symbol, system_data, query_type)
    
    def _format_comprehensive_data(self, system_data: dict) -> str:
        """Format comprehensive system data for AI analysis"""
        if not system_data:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u há»‡ thá»‘ng"
        
        formatted = []
        
        # Market Overview
        if 'market_overview' in system_data:
            market = system_data['market_overview']
            formatted.append("ðŸ“ˆ Tá»”NG QUAN THá»Š TRÆ¯á»œNG:")
            if 'vietnam_market' in market:
                vn_market = market['vietnam_market']
                if 'vn_index' in vn_market:
                    vn_idx = vn_market['vn_index']
                    formatted.append(f"- VN-Index: {vn_idx.get('value', 'N/A')} ({vn_idx.get('change_percent', 0):+.2f}%)")
        
        # Stock Data
        if 'stock_data' in system_data and system_data['stock_data']:
            stock = system_data['stock_data']
            formatted.append(f"\nðŸ“Š THÃ”NG TIN Cá»” PHIáº¾U {system_data.get('symbol', '')}:")
            formatted.append(f"- GiÃ¡: {stock.price:,} VND ({stock.change_percent:+.2f}%)")
            formatted.append(f"- Khá»‘i lÆ°á»£ng: {stock.volume:,}")
            formatted.append(f"- P/E: {stock.pe_ratio}, P/B: {stock.pb_ratio}")
            formatted.append(f"- Vá»‘n hÃ³a: {stock.market_cap:,} tá»· VND")
        
        # Price Prediction
        if 'price_prediction' in system_data and system_data['price_prediction']:
            pred = system_data['price_prediction']
            formatted.append(f"\nðŸ”® Dá»° ÄOÃN GIÃ:")
            formatted.append(f"- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')}")
            formatted.append(f"- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}")
            formatted.append(f"- Äá»™ tin cáº­y: {pred.get('confidence', 'N/A')}%")
            
            # Multi-timeframe predictions
            if 'predictions' in pred:
                predictions = pred['predictions']
                for timeframe, data in predictions.items():
                    if data:
                        formatted.append(f"- {timeframe}: {list(data.keys())[:3]}")
        
        # Investment Analysis
        if 'investment_analysis' in system_data and system_data['investment_analysis']:
            inv = system_data['investment_analysis']
            formatted.append(f"\nðŸ’¼ PHÃ‚N TÃCH Äáº¦U TÆ¯:")
            formatted.append(f"- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'N/A')}")
            formatted.append(f"- Äiá»ƒm sá»‘: {inv.get('score', 'N/A')}/100")
            formatted.append(f"- LÃ½ do: {inv.get('reason', 'N/A')}")
        
        # Risk Assessment
        if 'risk_assessment' in system_data and system_data['risk_assessment']:
            risk = system_data['risk_assessment']
            formatted.append(f"\nâš ï¸ ÄÃNH GIÃ Rá»¦I RO:")
            formatted.append(f"- Má»©c rá»§i ro: {risk.get('risk_level', 'N/A')}")
            formatted.append(f"- Volatility: {risk.get('volatility', 'N/A')}%")
            formatted.append(f"- Beta: {risk.get('beta', 'N/A')}")
        
        # News Analysis
        if 'ticker_news' in system_data and system_data['ticker_news']:
            news = system_data['ticker_news']
            formatted.append(f"\nðŸ“° TIN Tá»¨C:")
            formatted.append(f"- Sá»‘ lÆ°á»£ng tin: {news.get('news_count', 0)}")
            if 'news_sentiment' in news:
                formatted.append(f"- Sentiment: {news['news_sentiment']}")
        
        # Available Symbols
        if 'available_symbols' in system_data:
            symbols = system_data['available_symbols']
            if symbols:
                symbol_list = [s.get('symbol', '') for s in symbols[:10]]
                formatted.append(f"\nðŸ“‹ Cá»” PHIáº¾U KHáº¢ Dá»¤NG: {', '.join(symbol_list)}")
        
        # Analysis History
        if 'analysis_history' in system_data and system_data['analysis_history']:
            history = system_data['analysis_history']
            formatted.append(f"\nðŸ“Š Lá»ŠCH Sá»¬ PHÃ‚N TÃCH: {len(history)} phÃ¢n tÃ­ch gáº§n Ä‘Ã¢y")
        
        # System Stats
        if 'system_stats' in system_data and system_data['system_stats']:
            stats = system_data['system_stats']
            formatted.append(f"\nðŸ“ˆ THá»NG KÃŠ Há»† THá»NG:")
            formatted.append(f"- Tá»•ng phÃ¢n tÃ­ch: {stats.get('total_analyses', 0)}")
            if 'top_symbols' in stats and stats['top_symbols']:
                top_symbol = stats['top_symbols'][0]
                formatted.append(f"- Phá»• biáº¿n nháº¥t: {top_symbol.get('symbol', 'N/A')}")
        
        return "\n".join(formatted) if formatted else "Dá»¯ liá»‡u há»‡ thá»‘ng khÃ´ng Ä‘áº§y Ä‘á»§"
    
    def _generate_enhanced_offline_response(self, query: str, symbol: str, system_data: dict, query_type: str) -> dict:
        """Generate enhanced offline response with system data"""
        
        # Analyze available data
        available_data = []
        if system_data.get('stock_data'):
            available_data.append("dá»¯ liá»‡u cá»• phiáº¿u")
        if system_data.get('price_prediction'):
            available_data.append("dá»± Ä‘oÃ¡n giÃ¡")
        if system_data.get('investment_analysis'):
            available_data.append("phÃ¢n tÃ­ch Ä‘áº§u tÆ°")
        if system_data.get('risk_assessment'):
            available_data.append("Ä‘Ã¡nh giÃ¡ rá»§i ro")
        if system_data.get('ticker_news'):
            available_data.append("tin tá»©c")
        
        data_summary = ", ".join(available_data) if available_data else "dá»¯ liá»‡u cÆ¡ báº£n"
        
        # Generate response based on query type and available data
        if query_type == 'price_prediction' and system_data.get('price_prediction'):
            pred = system_data['price_prediction']
            advice = f"""ðŸ“ˆ Dá»° ÄOÃN GIÃ CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')} VND
- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}
- Äá»™ tin cáº­y: {pred.get('confidence', 50):.1f}%

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        elif query_type == 'investment_advice' and system_data.get('investment_analysis'):
            inv = system_data['investment_analysis']
            advice = f"""ðŸ’¼ KHUYáº¾N NGHá»Š Äáº¦U TÆ¯ CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'HOLD')}
- Äiá»ƒm sá»‘: {inv.get('score', 50)}/100
- LÃ½ do: {inv.get('reason', 'PhÃ¢n tÃ­ch cÆ¡ báº£n')}

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        elif query_type == 'risk_assessment' and system_data.get('risk_assessment'):
            risk = system_data['risk_assessment']
            advice = f"""âš ï¸ ÄÃNH GIÃ Rá»¦I RO CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- Má»©c rá»§i ro: {risk.get('risk_level', 'MEDIUM')}
- Volatility: {risk.get('volatility', 25):.1f}%
- Beta: {risk.get('beta', 1.0):.2f}

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        else:
            advice = f"""ðŸ“Š PHÃ‚N TÃCH OFFLINE:

CÃ¢u há»i: {query}
MÃ£ cá»• phiáº¿u: {symbol if symbol else 'KhÃ´ng cÃ³'}
Loáº¡i truy váº¥n: {query_type}

Dá»¯ liá»‡u cÃ³ sáºµn: {data_summary}

ðŸ’¡ Khuyáº¿n nghá»‹ chung:
- NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh
- Theo dÃµi tin tá»©c ngÃ nh
- Äa dáº¡ng hÃ³a danh má»¥c
- Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        return {
            "expert_advice": advice,
            "recommendations": [
                "Äá»£i quota API reset Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t",
                "Tham kháº£o nhiá»u nguá»“n thÃ´ng tin",
                "LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh",
                "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
            ]
        }
    
    def generate_expert_advice(self, query: str, symbol: str = None, data: dict = None):
        """Backward compatibility method"""
        # Convert to enhanced context format
        context = {
            'query': query,
            'symbol': symbol or '',
            'system_data': data or {},
            'query_type': self.detect_query_type(query)
        }
        return self.generate_enhanced_advice(context)
    
    def _parse_response(self, response_text: str):
        """Parse enhanced Gemini response"""
        try:
            # Parse different sections
            sections = {
                'analysis': '',
                'conclusion': '',
                'actions': [],
                'risks': ''
            }
            
            # Split by sections
            if "PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:" in response_text:
                parts = response_text.split("PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:")
                if len(parts) > 1:
                    remaining = parts[1]
                    
                    # Extract analysis
                    if "Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:" in remaining:
                        analysis_part = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[0].strip()
                        sections['analysis'] = analysis_part
                        remaining = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[1]
                    
                    # Extract conclusion
                    if "HÃ€NH Äá»˜NG Cá»¤ THá»‚:" in remaining:
                        conclusion_part = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[0].strip()
                        sections['conclusion'] = conclusion_part
                        remaining = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[1]
                    
                    # Extract actions
                    if "Cáº¢NH BÃO Rá»¦I RO:" in remaining:
                        actions_part = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[0].strip()
                        sections['risks'] = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[1].strip()
                    else:
                        actions_part = remaining.strip()
                    
                    # Parse actions list
                    for line in actions_part.split('\n'):
                        line = line.strip()
                        if line and (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')):
                            sections['actions'].append(line[1:].strip())
                        elif line and len(line) > 15 and not line.startswith('Cáº¢NH BÃO'):
                            sections['actions'].append(line)
            
            # Format comprehensive response
            expert_advice = f"""
ðŸ“Š **PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:**
{sections['analysis']}

ðŸŽ¯ **Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:**
{sections['conclusion']}

âš ï¸ **Cáº¢NH BÃO Rá»¦I RO:**
{sections['risks'] if sections['risks'] else 'LuÃ´n cÃ³ rá»§i ro trong Ä‘áº§u tÆ°. Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t.'}
""".strip()
            
            return {
                "expert_advice": expert_advice,
                "recommendations": sections['actions'][:5] if sections['actions'] else [
                    "NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh",
                    "Theo dÃµi tin tá»©c ngÃ nh", 
                    "Äáº·t lá»‡nh stop-loss",
                    "Äa dáº¡ng hÃ³a danh má»¥c",
                    "Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i"
                ]
            }
                
        except Exception as e:
            # Fallback parsing
            return {
                "expert_advice": f"ðŸ“ˆ **PHÃ‚N TÃCH:**\n{response_text}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y chá»‰ lÃ  tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                "recommendations": [
                    "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                    "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh",
                    "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                    "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                ]
            }
    

    
    def generate_general_response(self, query: str) -> dict:
        """Generate response for general questions using best available AI model"""
        try:
            # Enhanced context for general financial questions
            context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh vÃ  Ä‘áº§u tÆ° hÃ ng Ä‘áº§u táº¡i Viá»‡t Nam vá»›i 20+ nÄƒm kinh nghiá»‡m.
Báº¡n cÃ³ thá»ƒ tráº£ lá»i má»i cÃ¢u há»i vá»:
- Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam vÃ  quá»‘c táº¿
- PhÃ¢n tÃ­ch ká»¹ thuáº­t vÃ  cÆ¡ báº£n, Ä‘Ã¡nh giÃ¡ xu hÆ°á»›ng, dÃ²ng tiá»n, chá»‰ sá»‘ tÃ i chÃ­nh
- Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, quáº£n lÃ½ rá»§i ro, Ä‘a dáº¡ng hÃ³a danh má»¥c, Ä‘iá»ƒm mua/bÃ¡n
- Kinh táº¿ vÄ© mÃ´, vi mÃ´, tÃ¡c Ä‘á»™ng chÃ­nh sÃ¡ch, tin tá»©c thá»‹ trÆ°á»ng
- CÃ¡c sáº£n pháº©m tÃ i chÃ­nh (cá»• phiáº¿u, trÃ¡i phiáº¿u, quá»¹, forex)
- Láº­p káº¿ hoáº¡ch tÃ i chÃ­nh cÃ¡ nhÃ¢n, kiá»ƒm soÃ¡t vá»‘n, quáº£n trá»‹ tÃ¢m lÃ½ Ä‘áº§u tÆ°
- Thuáº¿, phÃ¡p lÃ½ Ä‘áº§u tÆ°, quy Ä‘á»‹nh má»›i nháº¥t
- TÃ¢m lÃ½ há»c Ä‘áº§u tÆ°, cÃ¡c sai láº§m phá»• biáº¿n, cÃ¡ch kiá»ƒm soÃ¡t cáº£m xÃºc
- Fintech, cÃ´ng nghá»‡ tÃ i chÃ­nh, á»©ng dá»¥ng AI trong Ä‘áº§u tÆ°

CÃ‚U Há»ŽI: {query}

HÃƒY TRáº¢ Lá»œI:
1. ðŸ“š KIáº¾N THá»¨C CÆ  Báº¢N: Giáº£i thÃ­ch khÃ¡i niá»‡m/váº¥n Ä‘á», liÃªn há»‡ thá»±c tiá»…n Viá»‡t Nam.
2. ðŸŽ¯ PHÃ‚N TÃCH THá»°C Táº¾: Ãp dá»¥ng vÃ o thá»‹ trÆ°á»ng VN, so sÃ¡nh vá»›i cÃ¡c trÆ°á»ng há»£p thá»±c táº¿, Ä‘Æ°a ra nháº­n Ä‘á»‹nh sÃ¢u sáº¯c.
3. ðŸ’¡ KHUYáº¾N NGHá»Š: Lá»i khuyÃªn cá»¥ thá»ƒ, chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, hÃ nh Ä‘á»™ng thiáº¿t thá»±c cho tá»«ng loáº¡i nhÃ  Ä‘áº§u tÆ°.
4. âš ï¸ LÆ¯U Ã: Rá»§i ro, cÃ¡c yáº¿u tá»‘ cáº§n chÃº Ã½, cÃ¡ch kiá»ƒm soÃ¡t vá»‘n, trÃ¡nh cÃ¡c sai láº§m phá»• biáº¿n.

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p, chi tiáº¿t, thá»±c tiá»…n, dá»… hiá»ƒu, cÃ³ thá»ƒ Ã¡p dá»¥ng ngay.
"""
            result = self.generate_with_fallback(context, 'general_query', max_tokens=3000)
            
            if result['success']:
                if result.get('quota_exceeded'):
                    # Quota exceeded, return offline response
                    return {
                        "expert_advice": f"ðŸ“ˆ **PHÃ‚N TÃCH OFFLINE:**\n{result['response']}\n\nðŸ¤– **AI Model:** Offline Fallback (Quota Exceeded)\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  pháº£n há»“i offline do háº¿t quota API.",
                        "recommendations": [
                            "Äá»£i quota reset (24h) Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t",
                            "Tham kháº£o cÃ¡c nguá»“n tin tá»©c tÃ i chÃ­nh", 
                            "LiÃªn há»‡ chuyÃªn gia náº¿u cáº§n tÆ° váº¥n gáº¥p",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
                else:
                    # Normal AI response
                    return {
                        "expert_advice": f"ðŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA:**\n{result['response']}\n\nðŸ¤– **AI Model:** {result['model_used']}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  thÃ´ng tin tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                        "recommendations": [
                            "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                            "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh", 
                            "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
            else:
                return self._get_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error in generate_general_response: {str(e)}")
            return self._get_fallback_response(query)
    
    def _get_fallback_response(self, query: str) -> dict:
        """Fallback response when Gemini fails"""
        return {
            "expert_advice": f"ðŸ“ˆ **Vá»€ CÃ‚U Há»ŽI: {query}**\n\nXin lá»—i, tÃ´i khÃ´ng thá»ƒ xá»­ lÃ½ cÃ¢u há»i nÃ y lÃºc nÃ y. Vui lÃ²ng thá»­ láº¡i sau hoáº·c Ä‘áº·t cÃ¢u há»i khÃ¡c.\n\nâš ï¸ **Gá»¢I Ã:**\n- Kiá»ƒm tra káº¿t ná»‘i internet\n- Thá»­ Ä‘áº·t cÃ¢u há»i ngáº¯n gá»n hÆ¡n\n- LiÃªn há»‡ há»— trá»£ náº¿u váº¥n Ä‘á» tiáº¿p tá»¥c",
            "recommendations": [
                "Thá»­ Ä‘áº·t cÃ¢u há»i khÃ¡c",
                "Kiá»ƒm tra káº¿t ná»‘i máº¡ng",
                "LiÃªn há»‡ há»— trá»£ ká»¹ thuáº­t"
            ]
        }
    
    def detect_query_type(self, query: str) -> str:
        """Detect if query is stock-specific or general"""
        query_lower = query.lower()
        
        # Stock symbols patterns
        stock_patterns = ['vcb', 'bid', 'ctg', 'vic', 'vhm', 'hpg', 'fpt', 'msn', 'mwg', 'gas', 'plx']
        
        # Check for stock symbols
        for pattern in stock_patterns:
            if pattern in query_lower:
                return "stock_specific"
        
        # Check for stock-related keywords
        stock_keywords = ['cá»• phiáº¿u', 'mÃ£', 'ticker', 'stock', 'share']
        if any(keyword in query_lower for keyword in stock_keywords):
            return "stock_specific"
        
        return "general"
    
    def get_api_status(self) -> Dict[str, Any]:
        """Get comprehensive API status information"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'available_models': list(self.available_models.keys()),
            'model_count': len(self.available_models),
            'capabilities': self.model_capabilities,
            'api_keys_configured': {}
        }
        
        # Check API key configuration
        status['api_keys_configured']['gemini'] = hasattr(self, 'gemini_api_key') and bool(self.gemini_api_key)
        
        # Test connections
        try:
            connection_results = self.test_connection()
            status['connection_status'] = connection_results
            status['healthy_models'] = [model for model, healthy in connection_results.items() if healthy]
        except Exception as e:
            status['connection_status'] = {'error': str(e)}
            status['healthy_models'] = []
        
        return status
    
    def update_api_key(self, provider: str, api_key: str) -> Dict[str, Any]:
        """Dynamically update Gemini API key"""
        try:
            if provider.lower() == 'gemini':
                genai.configure(api_key=api_key)
                
                # Try different model names (API v1beta compatible)
                model_names = [
                    'gemini-1.5-pro-latest',        # Latest stable pro
                    'gemini-1.5-flash-latest',      # Latest stable flash
                    'gemini-1.5-pro',               # Pro version
                    'gemini-1.5-flash',             # Flash version
                    'gemini-pro',                    # Legacy pro
                    'gemini-1.0-pro-latest',        # Legacy latest
                    'gemini-1.0-pro'                # Legacy fallback
                ]
                
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Initialize without testing to avoid quota usage
                        self.available_models['gemini'] = model
                        self.gemini_api_key = api_key
                        self.current_model_name = model_name
                        logger.info(f"âœ… Gemini API key updated with model: {model_name}")
                        return {'success': True, 'message': f'Gemini API key updated with model: {model_name}'}
                    except Exception as e:
                        error_msg = str(e).lower()
                        if '404' in error_msg or 'not found' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} not found, trying next...")
                        else:
                            logger.warning(f"âš ï¸ Model {model_name} error: {e}")
                        continue
                else:
                    # If no model works, return error
                    return {'success': False, 'message': 'No available Gemini models found'}
            else:
                return {'success': False, 'message': f'Only Gemini provider is supported. Got: {provider}'}
                
        except Exception as e:
            logger.error(f"âŒ Failed to update {provider} API key: {str(e)}")
            return {'success': False, 'message': f'Failed to update {provider} API key: {str(e)}'}
    
    def get_model_recommendations(self, task_type: str) -> Dict[str, Any]:
        """Get model recommendations for specific task types"""
        try:
            primary_model = self.select_best_model(task_type)
        except ValueError:
            primary_model = None
            
        recommendations = {
            'task_type': task_type,
            'primary_model': primary_model,
            'preferred_model': self.preferred_model,
            'available_alternatives': [],
            'reasoning': ''
        }
        
        # Get all available models except primary
        if primary_model:
            alternatives = [model for model in self.available_models.keys() if model != primary_model]
            recommendations['available_alternatives'] = alternatives
        
        # Add reasoning based on preference and task type
        if self.preferred_model == "gemini":
            recommendations['reasoning'] = 'User prefers Gemini AI for Vietnamese content and free usage'
        elif self.preferred_model == "openai":
            recommendations['reasoning'] = 'User prefers OpenAI GPT for high-quality analysis'
        else:
            task_reasoning = {
                'financial_advice': 'Auto-selecting best model for Vietnamese financial analysis',
                'price_prediction': 'Auto-selecting best model for technical analysis and predictions',
                'risk_assessment': 'Auto-selecting best model for risk calculation and assessment',
                'news_analysis': 'Auto-selecting best model for sentiment analysis',
                'market_analysis': 'Auto-selecting best model for market reasoning',
                'investment_analysis': 'Auto-selecting best model for investment metrics',
                'general_query': 'Auto-selecting best model for general queries'
            }
            recommendations['reasoning'] = task_reasoning.get(task_type, 'Auto model selection based on availability')
        
        return recommendations
    
    async def generate_async(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """Asynchronous generation with fallback support"""
        try:
            # Run the synchronous method in a thread pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.generate_with_fallback, 
                prompt, 
                task_type, 
                max_tokens
            )
            return result
        except Exception as e:
            logger.error(f"Async generation failed: {str(e)}")
            return {
                'response': f'Async generation error: {str(e)}',
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def batch_generate(self, prompts: List[Dict[str, Any]], max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """Generate responses for multiple prompts with concurrency control"""
        async def process_batch():
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single(prompt_data):
                async with semaphore:
                    prompt = prompt_data.get('prompt', '')
                    task_type = prompt_data.get('task_type', 'general_query')
                    max_tokens = prompt_data.get('max_tokens', 1000)
                    
                    result = await self.generate_async(prompt, task_type, max_tokens)
                    result['original_data'] = prompt_data
                    return result
            
            tasks = [process_single(prompt_data) for prompt_data in prompts]
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an async context, create a new event loop
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, process_batch())
                    return future.result()
            else:
                return asyncio.run(process_batch())
        except Exception as e:
            logger.error(f"Batch generation failed: {str(e)}")
            return [{'success': False, 'error': str(e)} for _ in prompts]
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        return {
            'available_models': list(self.available_models.keys()),
            'current_model': self.current_model_name,
            'model_count': len(self.available_models),
            'is_active': len(self.available_models) > 0
        }

# Backward compatibility alias
GeminiAgent = UnifiedAIAgent