import google.generativeai as genai
from openai import OpenAI
import os
import logging
from typing import Dict, Any, Optional, List
import asyncio
import json
import time
from datetime import datetime

try:
    from litellm import completion
    LITELLM_AVAILABLE = True
except ImportError:
    LITELLM_AVAILABLE = False
    completion = None

logger = logging.getLogger(__name__)
class UnifiedAIAgent:
    def __init__(self, gemini_api_key: str = None, openai_api_key: str = None, preferred_model: str = "auto"):
        """
        Initialize AI Agent with Gemini AI, OpenAI, and Llama
        """
        self.available_models = {}
        self.current_model_name = None
        self.preferred_model = preferred_model
        self.openai_client = None
        self.model_capabilities = {
            'gemini': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'risk_assessment'],
                'speed': 'fast',
                'cost': 'free'
            },
            'openai': {
                'strengths': ['analysis', 'reasoning', 'financial_advice', 'prediction', 'technical_analysis', 'news_analysis', 'english'],
                'speed': 'medium',
                'cost': 'paid'
            },
            'llama': {
                'strengths': ['analysis', 'vietnamese', 'reasoning', 'financial_advice', 'local_processing'],
                'speed': 'medium',
                'cost': 'free_local'
            }
        }
        
        # Initialize AI models with user-provided API keys
        # No hardcoded or environment variables used
        
        # Initialize Llama (local)
        if LITELLM_AVAILABLE:
            try:
                # Test Llama availability
                test_response = completion(
                    model="ollama/llama3.1:8b",
                    messages=[{"role": "user", "content": "test"}],
                    temperature=0.1,
                    max_tokens=10,
                )
                if test_response and test_response.choices:
                    self.available_models['llama'] = 'ollama/llama3.1:8b'
                    logger.info("âœ… Llama 3.1:8b initialized (local)")
            except Exception as e:
                logger.warning(f"âš ï¸ Llama not available: {str(e)}")
        else:
            logger.warning("âš ï¸ litellm not installed, Llama unavailable")
        
        # Initialize OpenAI
        if openai_api_key:
            try:
                self.openai_client = OpenAI(api_key=openai_api_key)
                self.openai_api_key = openai_api_key
                
                # Try different OpenAI models
                openai_models = [
                    'gpt-4o',           # Latest GPT-4 Omni
                    'gpt-4-turbo',      # GPT-4 Turbo
                    'gpt-4',            # Standard GPT-4
                    'gpt-3.5-turbo'     # Fallback
                ]
                
                # Just set the first available model without testing to avoid API calls during init
                self.available_models['openai'] = openai_models[0]
                logger.info(f"âœ… OpenAI initialized with model: {openai_models[0]}")
                        
            except Exception as e:
                logger.error(f"âŒ Failed to initialize OpenAI: {str(e)}")
        
        # Initialize Gemini
        if gemini_api_key:
            try:
                genai.configure(api_key=gemini_api_key)
                
                # Try different model names (API v1beta compatible)
                model_names = [
                    'gemini-3-pro-preview',        # Flagship má»›i nháº¥t, kháº£ nÄƒng suy luáº­n cao
                    'gemini-3-flash-preview',      # Tá»‘c Ä‘á»™ cao tháº¿ há»‡ 3

                        # --- GEMINI 2.5 SERIES (TiÃªu chuáº©n - Production Ready) ---
                    'gemini-2.5-pro',              # Báº£n chuáº©n tá»‘t nháº¥t cho má»i tÃ¡c vá»¥ (thay tháº¿ 1.5 Pro)
                    'gemini-2.5-flash',            # Báº£n chuáº©n tá»‘c Ä‘á»™ cao (thay tháº¿ 1.5 Flash)
                    'gemini-2.5-flash-lite',       # Chi phÃ­ cá»±c tháº¥p, tá»‘i Æ°u cho tÃ¡c vá»¥ Ä‘Æ¡n giáº£n
                    'gemini-2.5-pro-001',          # Báº£n snapshot cá»¥ thá»ƒ (trÃ¡nh thay Ä‘á»•i ngáº§m)
                    'gemini-2.5-flash-001',        # Báº£n snapshot cá»¥ thá»ƒ

                    # --- GEMINI 2.0 SERIES (á»”n Ä‘á»‹nh cÅ©) ---
                    'gemini-2.0-flash',            # Fallback tin cáº­y
                    'gemini-2.0-flash-exp',        # Báº£n thá»­ nghiá»‡m cÅ© (cÃ³ thá»ƒ váº«n hoáº¡t Ä‘á»™ng)

                    # --- LEGACY (CÅ© - Háº¡n cháº¿ dÃ¹ng cho dá»± Ã¡n má»›i) ---
                    'gemini-1.5-pro',
                    'gemini-1.5-flash',
                    'gemini-1.5-flash-8b',         # Báº£n siÃªu nháº¹ Ä‘á»i cÅ©

                    # --- SPECIALIZED (NhÃºng & HÃ¬nh áº£nh) ---
                    'text-embedding-005',          # Model nhÃºng vÄƒn báº£n má»›i nháº¥t (Semantic Search)
                    'imagen-3.0-generate-001',     # Táº¡o áº£nh
                    'aqa'             # Legacy fallback
                ]
                
                model_initialized = False
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Initialize without testing to avoid quota usage
                        self.available_models['gemini'] = model
                        self.gemini_api_key = gemini_api_key
                        self.current_model_name = model_name
                        logger.info(f"âœ… Gemini AI initialized with model: {model_name}")
                        model_initialized = True
                        break
                    except Exception as e:
                        error_msg = str(e).lower()
                        if 'quota' in error_msg or '429' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} quota exceeded, trying next...")
                        elif '404' in error_msg or 'not found' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} not found, trying next...")
                        else:
                            logger.warning(f"âš ï¸ Model {model_name} error: {e}")
                        continue
                
                if not model_initialized:
                    # If no model works, still allow offline mode
                    logger.warning("âš ï¸ No Gemini models available, will use offline mode")
                    self.available_models = {}
                    
            except Exception as e:
                logger.error(f"âŒ Failed to initialize Gemini: {str(e)}")
                # Don't set available_models if initialization failed
                self.available_models = {}
        
        # Set offline mode based on available models
        if not self.available_models:
            logger.warning("âš ï¸ No AI models available, system will run in offline mode")
            self.offline_mode = True
        else:
            self.offline_mode = False
            logger.info(f"âœ… AI models available: {list(self.available_models.keys())}")
    
    def test_connection(self):
        """Test AI API connections without using quota"""
        results = {}
        
        # Test Gemini
        if 'gemini' in self.available_models:
            try:
                # Just check if model exists, don't make API call
                if hasattr(self, 'gemini_api_key') and self.gemini_api_key:
                    results['gemini'] = True
                    logger.info("âœ… Gemini model available")
                else:
                    results['gemini'] = False
            except:
                results['gemini'] = False
        else:
            results['gemini'] = False
        
        # Test OpenAI
        if 'openai' in self.available_models:
            try:
                # Just check if client exists, don't make API call
                if hasattr(self, 'openai_client') and self.openai_client:
                    results['openai'] = True
                    logger.info("âœ… OpenAI model available")
                else:
                    results['openai'] = False
            except:
                results['openai'] = False
        else:
            results['openai'] = False
        
        # Test Llama
        if 'llama' in self.available_models:
            try:
                # Just check if litellm is available
                if LITELLM_AVAILABLE:
                    results['llama'] = True
                    logger.info("âœ… Llama model available")
                else:
                    results['llama'] = False
            except:
                results['llama'] = False
        else:
            results['llama'] = False
        
        return results
    
    def select_best_model(self, task_type: str) -> str:
        """
        Select the best available model for a specific task type based on user preference
        """
        # Respect user preference first
        if self.preferred_model == "gemini" and 'gemini' in self.available_models:
            return 'gemini'
        elif self.preferred_model == "openai" and 'openai' in self.available_models:
            return 'openai'
        elif self.preferred_model == "llama" and 'llama' in self.available_models:
            return 'llama'
        elif self.preferred_model == "auto":
            # Auto mode: prefer Gemini for Vietnamese content and free usage
            if 'gemini' in self.available_models:
                return 'gemini'
            # Fallback to OpenAI for English content
            if 'openai' in self.available_models:
                return 'openai'
            # Final fallback to Llama (local)
            if 'llama' in self.available_models:
                return 'llama'
        
        # Final fallback - use any available model (priority order)
        if 'gemini' in self.available_models:
            return 'gemini'
        if 'openai' in self.available_models:
            return 'openai'
        if 'llama' in self.available_models:
            return 'llama'
        
        raise ValueError("No AI models available")
    
    def generate_with_model(self, prompt: str, model_name: str, max_tokens: int = 2000) -> str:
        """
        Generate response using specified AI model
        """
        try:
            if model_name == 'gemini' and 'gemini' in self.available_models:
                response = self.available_models['gemini'].generate_content(prompt)
                return response.text
            
            elif model_name == 'openai' and 'openai' in self.available_models:
                if not hasattr(self, 'openai_client') or not self.openai_client:
                    raise ValueError("OpenAI client not initialized")
                openai_model = self.available_models['openai']
                response = self.openai_client.chat.completions.create(
                    model=openai_model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.7
                )
                return response.choices[0].message.content
            
            elif model_name == 'llama' and 'llama' in self.available_models:
                if not LITELLM_AVAILABLE:
                    raise ValueError("litellm not available for Llama")
                
                response = completion(
                    model="ollama/llama3.1:8b",
                    messages=[
                        {
                            "role": "system",
                            "content": "Trá»£ lÃ½ tÃ i chÃ­nh. Tráº£ lá»i ngáº¯n gá»n tiáº¿ng Viá»‡t."
                        },
                        {"role": "user", "content": prompt[:500]}  # Further limit prompt
                    ],
                    temperature=0.1,
                    max_tokens=min(max_tokens, 150),  # Further reduce tokens
                    timeout=10  # Reduce timeout
                )
                return response.choices[0].message.content

            else:
                raise ValueError(f"Model {model_name} not available.")
                
        except Exception as e:
            logger.error(f"Error generating with {model_name}: {str(e)}")
            raise

    def generate_with_fallback(self, prompt: str, task_type: str, max_tokens: int = 2000, force_model: str = None) -> Dict[str, Any]:
        """
        Generate response with automatic fallback respecting user preference
        """
        # Check if we're already in offline mode
        if getattr(self, 'offline_mode', True) or not self.available_models:
            logger.info("ğŸ“´ Using offline mode (no AI models available)")
            return self._generate_offline_fallback(prompt, task_type)
        
        try:
            # Use forced model if specified, otherwise use preferred model
            if force_model and force_model in self.available_models:
                target_model = force_model
                logger.info(f"ğŸ¯ Using forced model: {target_model}")
            else:
                target_model = self.select_best_model(task_type)
                logger.info(f"âš¡ Using selected model: {target_model}")
            
            response = self.generate_with_model(prompt, target_model, max_tokens)
            return {
                'response': response,
                'model_used': target_model,
                'success': True
            }
        except Exception as e:
            logger.error(f"Target model {target_model} failed: {str(e)}")
            
            # If force_model is specified and fails, return offline immediately
            if force_model:
                logger.info(f"Force model {force_model} failed, using offline mode")
                return self._generate_offline_fallback(prompt, task_type)
            
            # Try fallback model only if not using force_model
            try:
                fallback_models = [m for m in self.available_models.keys() if m != target_model]
                if fallback_models:
                    fallback_model = fallback_models[0]
                    logger.info(f"Trying fallback model: {fallback_model}")
                    response = self.generate_with_model(prompt, fallback_model, max_tokens)
                    return {
                        'response': response,
                        'model_used': fallback_model,
                        'success': True
                    }
            except Exception as e2:
                logger.error(f"Fallback model {fallback_models[0] if fallback_models else 'none'} failed: {str(e2)}")
            
            # Use offline fallback when all AI models fail
            logger.info("All models failed, using offline mode")
            return self._generate_offline_fallback(prompt, task_type)
    
    def _generate_offline_fallback(self, prompt: str, task_type: str) -> Dict[str, Any]:
        """
        Generate offline fallback response when API quota is exhausted
        """
        try:
            # Extract key information from prompt
            if 'CÃ‚U Há»I:' in prompt:
                question = prompt.split('CÃ‚U Há»I:')[1].split('MÃƒ Cá»” PHIáº¾U:')[0].strip()
            else:
                question = prompt[:200] + '...' if len(prompt) > 200 else prompt
            
            # Generate contextual offline response based on task type
            if task_type == 'financial_advice':
                response = self._generate_financial_advice_fallback(question)
            elif task_type == 'general_query':
                response = self._generate_general_fallback(question)
            else:
                response = self._generate_default_fallback(question)
            
            return {
                'response': response,
                'model_used': 'offline_mode',
                'success': True,
                'quota_exceeded': True
            }
        except Exception as e:
            logger.error(f"Offline fallback error: {e}")
            return {
                'response': 'Há»‡ thá»‘ng Ä‘ang báº£o trÃ¬. Vui lÃ²ng thá»­ láº¡i sau.',
                'model_used': 'offline_mode',
                'success': True,
                'quota_exceeded': True
            }
    
    def _generate_financial_advice_fallback(self, question: str) -> str:
        """
        Generate financial advice fallback when API quota exceeded
        """
        return f"""
PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
Do Gemini API Ä‘Ã£ háº¿t quota, há»‡ thá»‘ng chuyá»ƒn sang cháº¿ Ä‘á»™ offline. ÄÃ¢y lÃ  phÃ¢n tÃ­ch chuyÃªn sÃ¢u dá»±a trÃªn nguyÃªn táº¯c Ä‘áº§u tÆ° thá»±c tiá»…n vÃ  kinh nghiá»‡m thá»‹ trÆ°á»ng:

ğŸ“Š **PhÃ¢n tÃ­ch ká»¹ thuáº­t:**
- Xem xÃ©t xu hÆ°á»›ng giÃ¡ 20-50 phiÃªn gáº§n nháº¥t, xÃ¡c Ä‘á»‹nh vÃ¹ng há»— trá»£/khÃ¡ng cá»±, khá»‘i lÆ°á»£ng giao dá»‹ch, chá»‰ bÃ¡o RSI/MACD.
- ÄÃ¡nh giÃ¡ dÃ²ng tiá»n, má»©c Ä‘á»™ biáº¿n Ä‘á»™ng, cÃ¡c tÃ­n hiá»‡u Ä‘áº£o chiá»u.

ğŸ’° **PhÃ¢n tÃ­ch cÆ¡ báº£n:**
- Äá»c bÃ¡o cÃ¡o tÃ i chÃ­nh, chÃº Ã½ doanh thu, lá»£i nhuáº­n, biÃªn lá»£i nhuáº­n, dÃ²ng tiá»n hoáº¡t Ä‘á»™ng.
- So sÃ¡nh P/E, P/B vá»›i trung bÃ¬nh ngÃ nh, xem xÃ©t tÄƒng trÆ°á»Ÿng EPS, ROE, ROA.
- ÄÃ¡nh giÃ¡ ban lÃ£nh Ä‘áº¡o, chiáº¿n lÆ°á»£c phÃ¡t triá»ƒn, vá»‹ tháº¿ cáº¡nh tranh.

ğŸ“° **Tin tá»©c & sá»± kiá»‡n:**
- Theo dÃµi cÃ¡c tin tá»©c áº£nh hÆ°á»Ÿng Ä‘áº¿n ngÃ nh, cá»• phiáº¿u, chÃ­nh sÃ¡ch vÄ© mÃ´, lÃ£i suáº¥t, tá»· giÃ¡.
- ÄÃ¡nh giÃ¡ tÃ¡c Ä‘á»™ng cá»§a cÃ¡c sá»± kiá»‡n Ä‘áº·c biá»‡t (chia cá»• tá»©c, phÃ¡t hÃ nh thÃªm, M&A).

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
- Chá»‰ Ä‘áº§u tÆ° khi hiá»ƒu rÃµ doanh nghiá»‡p, ngÃ nh vÃ  xu hÆ°á»›ng thá»‹ trÆ°á»ng.
- Äáº·t má»¥c tiÃªu lá»£i nhuáº­n, Ä‘iá»ƒm cáº¯t lá»— rÃµ rÃ ng.
- Äa dáº¡ng hÃ³a danh má»¥c, khÃ´ng dá»“n vá»‘n vÃ o má»™t mÃ£.
- LuÃ´n cáº­p nháº­t thÃ´ng tin, Ä‘iá»u chá»‰nh chiáº¿n lÆ°á»£c khi cÃ³ biáº¿n Ä‘á»™ng lá»›n.

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- Äá»c ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh quÃ½ gáº§n nháº¥t.
- Láº­p báº£ng so sÃ¡nh cÃ¡c mÃ£ cÃ¹ng ngÃ nh.
- Theo dÃµi diá»…n biáº¿n thá»‹ trÆ°á»ng hÃ ng ngÃ y.
- Äáº·t lá»‡nh stop-loss, take-profit cho tá»«ng vá»‹ tháº¿.
- Tham kháº£o Ã½ kiáº¿n chuyÃªn gia, cá»™ng Ä‘á»“ng Ä‘áº§u tÆ°.

Cáº¢NH BÃO Rá»¦I RO:
âš ï¸ **QUAN TRá»ŒNG:** ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline cÆ¡ báº£n do háº¿t quota API. 
KhÃ´ng nÃªn dá»±a vÃ o Ä‘Ã¢y Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ° quan trá»ng.
HÃ£y Ä‘á»£i API reset hoáº·c tham kháº£o chuyÃªn gia tÃ i chÃ­nh.
"""
    
    def _generate_general_fallback(self, question: str) -> str:
        """
        Generate comprehensive general query fallback with smart question analysis
        """
        question_lower = question.lower()
        
        # Specific stock analysis questions
        if any(stock in question_lower for stock in ['vcb', 'hpg', 'vic', 'vhm']):
            return self._generate_stock_specific_advice(question)
        elif any(word in question_lower for word in ['so sÃ¡nh', 'compare', 'tá»‘t hÆ¡n']):
            return self._generate_comparison_advice(question)
        elif any(word in question_lower for word in ['ngÆ°á»i má»›i', 'báº¯t Ä‘áº§u', 'beginner']):
            return self._generate_beginner_advice()
        elif any(word in question_lower for word in ['rá»§i ro', 'risk', 'quáº£n lÃ½']):
            return self._generate_risk_management_advice()
        elif any(word in question_lower for word in ['Ä‘áº§u tÆ°', 'investment', 'chiáº¿n lÆ°á»£c']):
            return self._generate_investment_strategy_advice()
        elif any(word in question_lower for word in ['phÃ¢n tÃ­ch', 'analysis', 'triá»ƒn vá»ng']):
            return self._generate_analysis_advice()
        elif any(word in question_lower for word in ['danh má»¥c', 'portfolio', 'Ä‘a dáº¡ng']):
            return self._generate_portfolio_advice()
        else:
            return f"""
ğŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA:**

**Vá» cÃ¢u há»i:** {question}

ğŸ’¡ **NguyÃªn táº¯c Ä‘áº§u tÆ° cÆ¡ báº£n:**

**1. ğŸ“Š NghiÃªn cá»©u trÆ°á»›c khi Ä‘áº§u tÆ°:**
- PhÃ¢n tÃ­ch bÃ¡o cÃ¡o tÃ i chÃ­nh: doanh thu, lá»£i nhuáº­n, ná»£ pháº£i tráº£
- ÄÃ¡nh giÃ¡ P/E, P/B, ROE, ROA so vá»›i trung bÃ¬nh ngÃ nh
- TÃ¬m hiá»ƒu vá» ban lÃ£nh Ä‘áº¡o vÃ  chiáº¿n lÆ°á»£c phÃ¡t triá»ƒn

**2. âš–ï¸ Quáº£n lÃ½ rá»§i ro:**
- Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i (khÃ´ng áº£nh hÆ°á»Ÿng sinh hoáº¡t)
- Äa dáº¡ng hÃ³a: khÃ´ng quÃ¡ 20% vá»‘n vÃ o má»™t mÃ£
- Äáº·t stop-loss: cáº¯t lá»— khi giáº£m 10-15%

**3. ğŸ¯ Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°:**
- XÃ¡c Ä‘á»‹nh má»¥c tiÃªu: ngáº¯n háº¡n (< 1 nÄƒm) hay dÃ i háº¡n (> 3 nÄƒm)
- Äáº§u tÆ° Ä‘á»‹nh ká»³ (DCA) Ä‘á»ƒ giáº£m rá»§i ro thá»i Ä‘iá»ƒm
- KiÃªn nháº«n vÃ  ká»· luáº­t vá»›i káº¿ hoáº¡ch Ä‘Ã£ Ä‘á» ra

**4. ğŸ“° Theo dÃµi thÃ´ng tin:**
- Tin tá»©c cÃ´ng ty vÃ  ngÃ nh
- ChÃ­nh sÃ¡ch kinh táº¿ vÄ© mÃ´
- Xu hÆ°á»›ng thá»‹ trÆ°á»ng toÃ n cáº§u

âš ï¸ **LÆ°u Ã½:** ÄÃ¢y lÃ  kiáº¿n thá»©c cÆ¡ báº£n. LuÃ´n tham kháº£o chuyÂªn gia tÃ i chÃ­nh trÆ°á»›c khi Ä‘áº§u tÆ°.
"""
    
    def _generate_stock_specific_advice(self, question: str) -> str:
        """Generate advice for specific stock questions"""
        question_lower = question.lower()
        
        if 'vcb' in question_lower:
            return """
ğŸ¦ **PHÃ‚N TÃCH VCB (Vietcombank):**

**ğŸ“Š Æ¯u Ä‘iá»ƒm:**
- NgÃ¢n hÃ ng lá»›n nháº¥t Viá»‡t Nam theo vá»‘n hÃ³a
- ThÆ°Æ¡ng hiá»‡u máº¡nh, máº¡ng lÆ°á»›i rá»™ng
- ROE á»•n Ä‘á»‹nh 18-22%, NIM khoáº£ng 3.5%
- Cá»• tá»©c háº¥p dáº«n 8-12%/nÄƒm

**âš ï¸ Rá»§i ro:**
- Nháº¡y cáº£m vá»›i chÃ­nh sÃ¡ch tiá»n tá»‡
- Cáº¡nh tranh gay gáº¯t trong ngÃ nh
- Rá»§i ro tÃ­n dá»¥ng khi kinh táº¿ suy giáº£m

**ğŸ¯ Khuyáº¿n nghá»‹:**
- PhÃ¹ há»£p Ä‘áº§u tÆ° dÃ i háº¡n (> 2 nÄƒm)
- Mua khi P/B < 2.0, P/E < 12
- Äáº·t stop-loss 10% dÆ°á»›i giÃ¡ mua
- Theo dÃµi lÃ£i suáº¥t vÃ  chÃ­nh sÃ¡ch NHNN

**ğŸ“ˆ Má»¥c tiÃªu giÃ¡:** 65,000-70,000 VND (6-12 thÃ¡ng)
"""
        elif 'hpg' in question_lower:
            return """
ğŸ¢ **PHÃ‚N TÃCH HPG (HÃ²a PhÃ¡t):**

**ğŸ“Š Æ¯u Ä‘iá»ƒm:**
- NhÃ  sáº£n xuáº¥t thÃ©p hÃ ng Ä‘áº§u Viá»‡t Nam
- CÃ´ng nghá»‡ hiá»‡n Ä‘áº¡i, chi phÃ­ cáº¡nh tranh
- HÆ°á»Ÿng lá»£i tá»« phÃ¡t triá»ƒn háº¡ táº§ng
- BiÃªn lá»£i nhuáº­n cáº£i thiá»‡n

**âš ï¸ Rá»§i ro:**
- Chu ká»³ ngÃ nh thÃ©p biáº¿n Ä‘á»™ng máº¡nh
- Phá»¥ thuá»™c giÃ¡ quáº·ng sáº¯t tháº¿ giá»›i
- Cáº¡nh tranh tá»« thÃ©p nháº­p kháº©u
- áº¢nh hÆ°á»Ÿng bá»Ÿi chÃ­nh sÃ¡ch mÃ´i trÆ°á»ng

**ğŸ¯ Khuyáº¿n nghá»‹:**
- Äáº§u tÆ° theo chu ká»³ ngÃ nh
- Mua khi P/E < 8, P/B < 1.5
- Theo dÃµi giÃ¡ quáº·ng sáº¯t vÃ  nhu cáº§u xÃ¢y dá»±ng
- Cáº©n tháº­n vá»›i biáº¿n Ä‘á»™ng ngáº¯n háº¡n

**ğŸ“ˆ Má»¥c tiÃªu giÃ¡:** 28,000-32,000 VND (6-12 thÃ¡ng)
"""
        else:
            return """
ğŸ“ˆ **PHÃ‚N TÃCH Cá»” PHIáº¾U Cá»¤ THá»‚:**

**ğŸ” CÃ¡c bÆ°á»›c phÃ¢n tÃ­ch:**
1. **Kiá»ƒm tra cÆ¡ báº£n:** P/E, P/B, ROE, tÄƒng trÆ°á»Ÿng
2. **ÄÃ¡nh giÃ¡ ngÃ nh:** Vá»‹ tháº¿ cáº¡nh tranh, triá»ƒn vá»ng
3. **PhÃ¢n tÃ­ch ká»¹ thuáº­t:** Xu hÆ°á»›ng, há»— trá»£/khÃ¡ng cá»±
4. **Quáº£n lÃ½ rá»§i ro:** Stop-loss, position size

**ğŸ¯ Quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ°:**
- **MUA:** Khi cÆ¡ báº£n tá»‘t + ká»¹ thuáº­t tÃ­ch cá»±c
- **GIá»®:** Khi cÆ¡ báº£n á»•n Ä‘á»‹nh + ká»¹ thuáº­t trung tÃ­nh
- **BÃN:** Khi cÆ¡ báº£n xáº¥u + ká»¹ thuáº­t tiÃªu cá»±c

âš ï¸ **LÆ°u Ã½:** LuÃ´n Ä‘á»c bÃ¡o cÃ¡o tÃ i chÃ­nh gáº§n nháº¥t trÆ°á»›c khi quyáº¿t Ä‘á»‹nh.
"""
    
    def _generate_comparison_advice(self, question: str) -> str:
        """Generate advice for comparison questions"""
        return """
ğŸ”„ **SO SÃNH Cá»” PHIáº¾U:**

**ğŸ“Š TiÃªu chÃ­ so sÃ¡nh:**

**1. Chá»‰ sá»‘ tÃ i chÃ­nh:**
- **P/E Ratio:** Tháº¥p hÆ¡n = háº¥p dáº«n hÆ¡n
- **ROE:** Cao hÆ¡n = hiá»‡u quáº£ tá»‘t hÆ¡n
- **Debt/Equity:** Tháº¥p hÆ¡n = an toÃ n hÆ¡n
- **Revenue Growth:** Cao hÆ¡n = tiá»m nÄƒng tá»‘t hÆ¡n

**2. Yáº¿u tá»‘ Ä‘á»‹nh tÃ­nh:**
- **Vá»‹ tháº¿ thá»‹ trÆ°á»ng:** Leader vs Follower
- **MÃ´ hÃ¬nh kinh doanh:** á»”n Ä‘á»‹nh vs Biáº¿n Ä‘á»™ng
- **Quáº£n lÃ½:** Kinh nghiá»‡m vÃ  Ä‘á»‹nh hÆ°á»›ng
- **Cá»• tá»©c:** Tá»· lá»‡ vÃ  Ä‘á»™ á»•n Ä‘á»‹nh

**3. Triá»ƒn vá»ng ngÃ nh:**
- **Chu ká»³ sá»‘ng:** Má»›i ná»•i vs TrÆ°á»Ÿng thÃ nh vs Suy giáº£m
- **Cáº¡nh tranh:** Má»©c Ä‘á»™ vÃ  rÃ o cáº£n gia nháº­p
- **Quy Ä‘á»‹nh:** TÃ¡c Ä‘á»™ng cá»§a chÃ­nh sÃ¡ch

**ğŸ¯ PhÆ°Æ¡ng phÃ¡p lá»±a chá»n:**
1. Láº­p báº£ng so sÃ¡nh cÃ¡c chá»‰ sá»‘ chÃ­nh
2. ÄÃ¡nh giÃ¡ Ä‘iá»ƒm máº¡nh/yáº¿u cá»§a tá»«ng mÃ£
3. XÃ©t Ä‘áº¿n má»¥c tiÃªu Ä‘áº§u tÆ° cá»§a báº¡n
4. Chá»n mÃ£ phÃ¹ há»£p vá»›i há»“ sÆ¡ rá»§i ro

ğŸ’¡ **Tip:** Äá»«ng chá»‰ so sÃ¡nh sá»‘ liá»‡u, hÃ£y hiá»ƒu rÃµ cÃ¢u chuyá»‡n kinh doanh!
"""
    
    def _generate_beginner_advice(self) -> str:
        """Generate advice for beginners"""
        return """
ğŸŒ± **CHIáº¾N LÆ¯á»¢C Äáº¦U TÆ¯ CHO NGÆ¯á»œI Má»šI:**

**ğŸ“š BÆ¯á»›c 1: Há»c kiáº¿n thá»©c cÆ¡ báº£n**
- Hiá»ƒu cÃ¡c chá»‰ sá»‘: P/E, P/B, ROE, EPS
- Náº¯m vá»¯ng phÃ¢n tÃ­ch cÆ¡ báº£n vÃ  ká»¹ thuáº­t
- Äá»c sÃ¡ch, tham gia khÃ³a há»c
- Theo dÃµi tin tá»©c tÃ i chÃ­nh hÃ ng ngÃ y

**ğŸ’° BÆ°á»›c 2: Chuáº©n bá»‹ tÃ i chÃ­nh**
- Chá»‰ dÃ¹ng tiá»n nhÃ n rá»—i (khÃ´ng áº£nh hÆ°á»Ÿng sinh hoáº¡t)
- Báº¯t Ä‘áº§u vá»›i 10-50 triá»‡u VND
- Má»Ÿ tÃ i khoáº£n chá»©ng khoÃ¡n táº¡i cÃ´ng ty uy tÃ­n
- CÃ³ quá»¹ dá»± phÃ²ng 6 thÃ¡ng chi phÃ­

**ğŸ¯ BÆ°á»›c 3: Chiáº¿n lÆ°á»£c Ä‘áº§u tiÃªn**
- Báº¯t Ä‘áº§u vá»›i blue-chip: VCB, VIC, VNM, GAS
- Äáº§u tÆ° Ä‘á»‹nh ká»³ (DCA) 2-5 triá»‡u/thÃ¡ng
- Äa dáº¡ng hÃ³a: 3-5 mÃ£ khÃ¡c ngÃ nh
- Äáº·t stop-loss 10-15%

**ğŸ” BÆ°á»›c 4: Theo dÃµi vÃ  há»c há»i**
- Ghi chÃ©p má»i giao dá»‹ch vÃ  lÃ½ do
- ÄÃ¡nh giÃ¡ káº¿t quáº£ hÃ ng thÃ¡ng
- Há»c tá»« sai láº§m, Ä‘iá»u chá»‰nh chiáº¿n lÆ°á»£c
- Tham gia cá»™ng Ä‘á»“ng Ä‘áº§u tÆ°

**âš ï¸ Sai láº§m cáº§n trÃ¡nh:**
- Äáº§u tÆ° theo tin Ä‘á»“n, FOMO
- KhÃ´ng Ä‘áº·t stop-loss
- Vay tiá»n Ä‘á»ƒ Ä‘áº§u tÆ°
- Mong muá»‘n lÃ m giÃ u nhanh
- Bá» qua viá»‡c há»c há»i

ğŸ’¡ **Nhá»›:** Äáº§u tÆ° lÃ  cuá»™c marathon, khÃ´ng pháº£i sprint!
"""
    
    def _generate_risk_management_advice(self) -> str:
        return """
ğŸ“ˆ **QUáº¢N LÃ Rá»¦I RO TRONG Äáº¦U TÆ¯ Cá»” PHIáº¾U:**

**1. ğŸ¯ NguyÃªn táº¯c cÆ¡ báº£n:**
- **Quy táº¯c 1-5-10:** KhÃ´ng máº¥t quÃ¡ 1% tÃ i khoáº£n/lá»‡nh, 5%/ngÃ y, 10%/thÃ¡ng
- **Äa dáº¡ng hÃ³a:** Tá»‘i thiá»ƒu 8-10 mÃ£ khÃ¡c ngÃ nh
- **Tá»· lá»‡ vá»‘n:** Cá»• phiáº¿u khÃ´ng quÃ¡ 70% tá»•ng tÃ i sáº£n

**2. âš–ï¸ CÃ´ng cá»¥ quáº£n lÃ½ rá»§i ro:**
- **Stop-loss:** Cáº¯t lá»— tá»± Ä‘á»™ng khi giáº£m 8-12%
- **Take-profit:** Chá»‘t lá»i khi Ä‘áº¡t má»¥c tiÃªu 15-25%
- **Position sizing:** TÃ­nh toÃ¡n sá»‘ lÆ°á»£ng cá»• phiáº¿u phÃ¹ há»£p

**3. ğŸ“Š ÄÃ¡nh giÃ¡ rá»§i ro:**
- **Beta:** Äo Ä‘á»™ biáº¿n Ä‘á»™ng so vá»›i thá»‹ trÆ°á»ng
- **Volatility:** Má»©c Ä‘á»™ dao Ä‘á»™ng giÃ¡
- **Drawdown:** Má»©c giáº£m tá»‘i Ä‘a tá»« Ä‘á»‰nh

**4. ğŸ›¡ï¸ Chiáº¿n lÆ°á»£c báº£o vá»‡:**
- **Hedge:** Sá»­ dá»¥ng derivatives Ä‘á»ƒ báº£o hiá»ƒm
- **Rebalancing:** CÃ¢n báº±ng láº¡i danh má»¥c Ä‘á»‹nh ká»³
- **Cash reserve:** Giá»¯ 20-30% tiá»n máº·t

**5. ğŸ§  TÃ¢m lÃ½ Ä‘áº§u tÆ°:**
- KhÃ´ng Ä‘áº§u tÆ° khi cáº£m xÃºc (sá»£ hÃ£i/tham lam)
- TuÃ¢n thá»§ káº¿ hoáº¡ch Ä‘Ã£ Ä‘á» ra
- Há»c há»i tá»« sai láº§m

ğŸ’¡ **CÃ´ng thá»©c tÃ­nh position size:**
Sá»‘ cá»• phiáº¿u = (Vá»‘n Ã— % rá»§i ro) Ã· (GiÃ¡ mua - Stop loss)

âš ï¸ **Nhá»›:** Rá»§i ro vÃ  lá»£i nhuáº­n luÃ´n Ä‘i Ä‘Ã´i. Quáº£n lÃ½ tá»‘t rá»§i ro = báº£o vá»‡ vá»‘n dÃ i háº¡n.
"""
    
    def _generate_investment_strategy_advice(self) -> str:
        return """
ğŸ“ˆ **CHIáº¾N LÆ¯á»¢C Äáº¦U TÆ¯ Cá»” PHIáº¾U:**

**1. ğŸ¯ XÃ¡c Ä‘á»‹nh má»¥c tiÃªu:**
- **Ngáº¯n háº¡n (< 1 nÄƒm):** Swing trading, lá»£i nhuáº­n 15-30%
- **Trung háº¡n (1-3 nÄƒm):** Growth investing, lá»£i nhuáº­n 50-100%
- **DÃ i háº¡n (> 3 nÄƒm):** Value investing, lá»£i nhuáº­n 100-300%

**2. ğŸ“Š PhÆ°Æ¡ng phÃ¡p phÃ¢n tÃ­ch:**
- **PhÃ¢n tÃ­ch cÆ¡ báº£n:** P/E, P/B, ROE, tÄƒng trÆ°á»Ÿng doanh thu
- **PhÃ¢n tÃ­ch ká»¹ thuáº­t:** MA, RSI, MACD, support/resistance
- **PhÃ¢n tÃ­ch vÄ© mÃ´:** GDP, láº¡m phÃ¡t, lÃ£i suáº¥t, tá»· giÃ¡

**3. ğŸ—ï¸ XÃ¢y dá»±ng danh má»¥c:**
- **Core (60%):** Cá»• phiáº¿u blue-chip á»•n Ä‘á»‹nh
- **Growth (25%):** Cá»• phiáº¿u tÄƒng trÆ°á»Ÿng cao
- **Speculative (15%):** Cá»• phiáº¿u tiá»m nÄƒng, rá»§i ro cao

**4. â° Thá»i Ä‘iá»ƒm vÃ o lá»‡nh:**
- **DCA (Dollar Cost Averaging):** Mua Ä‘á»‹nh ká»³
- **Value averaging:** Mua nhiá»u khi giÃ¡ tháº¥p
- **Momentum:** Mua khi xu hÆ°á»›ng tÄƒng rÃµ rÃ ng

**5. ğŸ”„ Quáº£n lÃ½ danh má»¥c:**
- **Rebalancing:** 3-6 thÃ¡ng/láº§n
- **Review:** ÄÃ¡nh giÃ¡ hiá»‡u suáº¥t hÃ ng quÃ½
- **Adjustment:** Äiá»u chá»‰nh theo thá»‹ trÆ°á»ng

**6. ğŸª Chiáº¿n lÆ°á»£c theo thá»‹ trÆ°á»ng:**
- **Bull market:** TÄƒng tá»· trá»ng cá»• phiáº¿u
- **Bear market:** Giáº£m tá»· trá»ng, tÄƒng tiá»n máº·t
- **Sideways:** Focus vÃ o cá»• tá»©c, trading ngáº¯n háº¡n

ğŸ’¡ **Tip:** Báº¯t Ä‘áº§u vá»›i sá»‘ tiá»n nhá», há»c kinh nghiá»‡m trÆ°á»›c khi tÄƒng vá»‘n.
"""
    
    def _generate_analysis_advice(self) -> str:
        return """
ğŸ“ˆ **PHÆ¯Æ NG PHÃP PHÃ‚N TÃCH Cá»” PHIáº¾U:**

**1. ğŸ“Š PhÃ¢n tÃ­ch cÆ¡ báº£n (Fundamental Analysis):**
- **BÃ¡o cÃ¡o tÃ i chÃ­nh:** Doanh thu, lá»£i nhuáº­n, ná»£, dÃ²ng tiá»n
- **Chá»‰ sá»‘ Ä‘á»‹nh giÃ¡:** P/E, P/B, P/S, EV/EBITDA
- **Chá»‰ sá»‘ hiá»‡u quáº£:** ROE, ROA, ROIC, profit margin
- **TÄƒng trÆ°á»Ÿng:** Revenue growth, EPS growth

**2. ğŸ“ˆ PhÃ¢n tÃ­ch ká»¹ thuáº­t (Technical Analysis):**
- **Xu hÆ°á»›ng:** Uptrend, downtrend, sideways
- **Support/Resistance:** VÃ¹ng há»— trá»£/khÃ¡ng cá»±
- **Chá»‰ bÃ¡o:** RSI, MACD, Bollinger Bands, MA
- **Patterns:** Head & shoulders, triangle, flag

**3. ğŸŒ PhÃ¢n tÃ­ch vÄ© mÃ´:**
- **Kinh táº¿:** GDP, láº¡m phÃ¡t, tháº¥t nghiá»‡p
- **ChÃ­nh sÃ¡ch:** LÃ£i suáº¥t, chÃ­nh sÃ¡ch tÃ i khÃ³a
- **NgÃ nh:** Chu ká»³ ngÃ nh, cáº¡nh tranh
- **Quá»‘c táº¿:** ThÆ°Æ¡ng máº¡i, Ä‘á»‹a chÃ­nh trá»‹

**4. ğŸ” Quy trÃ¬nh phÃ¢n tÃ­ch:**
- **BÆ°á»›c 1:** PhÃ¢n tÃ­ch vÄ© mÃ´ â†’ chá»n ngÃ nh
- **BÆ°á»›c 2:** So sÃ¡nh cÃ¡c cÃ´ng ty trong ngÃ nh
- **BÆ°á»›c 3:** PhÃ¢n tÃ­ch cÆ¡ báº£n cÃ´ng ty
- **BÆ°á»›c 4:** PhÃ¢n tÃ­ch ká»¹ thuáº­t timing
- **BÆ°á»›c 5:** ÄÃ¡nh giÃ¡ rá»§i ro/lá»£i nhuáº­n

**5. ğŸ¯ Chá»‰ sá»‘ quan trá»ng:**
- **P/E < 15:** CÃ³ thá»ƒ undervalued
- **ROE > 15%:** Hiá»‡u quáº£ sá»­ dá»¥ng vá»‘n tá»‘t
- **Debt/Equity < 0.5:** Cáº¥u trÃºc tÃ i chÃ­nh lÃ nh máº¡nh
- **Revenue growth > 10%:** TÄƒng trÆ°á»Ÿng tá»‘t

ğŸ’¡ **LÆ°u Ã½:** Káº¿t há»£p cáº£ 3 phÆ°Æ¡ng phÃ¡p Ä‘á»ƒ cÃ³ quyáº¿t Ä‘á»‹nh Ä‘áº§u tÆ° tá»‘i Æ°u.
"""
    
    def _generate_portfolio_advice(self) -> str:
        return """
ğŸ“ˆ **XÃ‚Y Dá»°NG DANH Má»¤C Äáº¦U TÆ¯:**

**1. ğŸ¯ NguyÃªn táº¯c Ä‘a dáº¡ng hÃ³a:**
- **Theo ngÃ nh:** Tá»‘i thiá»ƒu 5-8 ngÃ nh khÃ¡c nhau
- **Theo vá»‘n hÃ³a:** Large-cap (60%), Mid-cap (25%), Small-cap (15%)
- **Theo Ä‘á»‹a lÃ½:** Trong nÆ°á»›c (70%), quá»‘c táº¿ (30%)
- **Theo tÃ i sáº£n:** Cá»• phiáº¿u, trÃ¡i phiáº¿u, vÃ ng, báº¥t Ä‘á»™ng sáº£n

**2. ğŸ“Š Cáº¥u trÃºc danh má»¥c máº«u:**
- **Báº£o thá»§ (Risk < 30%):** 40% cá»• phiáº¿u, 50% trÃ¡i phiáº¿u, 10% tiá»n máº·t
- **CÃ¢n báº±ng (Risk 30-70%):** 60% cá»• phiáº¿u, 30% trÃ¡i phiáº¿u, 10% khÃ¡c
- **TÃ­ch cá»±c (Risk > 70%):** 80% cá»• phiáº¿u, 15% trÃ¡i phiáº¿u, 5% khÃ¡c

**3. ğŸ—ï¸ XÃ¢y dá»±ng tá»«ng táº§ng:**
- **Táº§ng 1 - Core (50%):** Blue-chip, cá»• tá»©c á»•n Ä‘á»‹nh
- **Táº§ng 2 - Satellite (30%):** Growth stocks, mid-cap
- **Táº§ng 3 - Speculative (20%):** Small-cap, emerging sectors

**4. âš–ï¸ CÃ¢n báº±ng danh má»¥c:**
- **Rebalancing:** 3-6 thÃ¡ng/láº§n
- **Threshold:** Khi tá»· trá»ng lá»‡ch > 5%
- **Calendar:** Cuá»‘i quÃ½/nÄƒm
- **Tactical:** Theo Ä‘iá»u kiá»‡n thá»‹ trÆ°á»ng

**5. ğŸ“ˆ Theo dÃµi hiá»‡u suáº¥t:**
- **Benchmark:** So vá»›i VN-Index, VN30
- **Risk-adjusted return:** Sharpe ratio, Sortino ratio
- **Drawdown:** Má»©c giáº£m tá»‘i Ä‘a
- **Volatility:** Äá»™ biáº¿n Ä‘á»™ng

**6. ğŸ”„ Äiá»u chá»‰nh theo chu ká»³:**
- **Bull market:** TÄƒng tá»· trá»ng cá»• phiáº¿u growth
- **Bear market:** TÄƒng defensive stocks, tiá»n máº·t
- **Recovery:** Focus vÃ o cyclical stocks

ğŸ’¡ **Quy táº¯c vÃ ng:** KhÃ´ng bao giá» Ä‘áº·t táº¥t cáº£ trá»©ng vÃ o má»™t giá»!
"""
    
    def _generate_default_fallback(self, question: str) -> str:
        """
        Generate default fallback response
        """
        return f"""
ğŸ¤– **Há»† THá»NG OFFLINE:**

Xin lá»—i, Gemini API Ä‘Ã£ háº¿t quota nÃªn tÃ´i khÃ´ng thá»ƒ phÃ¢n tÃ­ch chi tiáº¿t lÃºc nÃ y.

**CÃ¢u há»i:** {question}

**Khuyáº¿n nghá»‹ thá»±c táº¿:**
- Thá»­ láº¡i sau vÃ i giá» khi quota reset.
- Äá»c bÃ¡o cÃ¡o tÃ i chÃ­nh, phÃ¢n tÃ­ch ká»¹ thuáº­t cÆ¡ báº£n.
- Tham kháº£o cÃ¡c nguá»“n thÃ´ng tin tÃ i chÃ­nh uy tÃ­n, cá»™ng Ä‘á»“ng Ä‘áº§u tÆ°.
- Láº­p káº¿ hoáº¡ch Ä‘áº§u tÆ° rÃµ rÃ ng, kiá»ƒm soÃ¡t rá»§i ro.
- LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh náº¿u cáº§n tÆ° váº¥n gáº¥p.

â° **Quota thÆ°á»ng reset sau 24 giá»**
"""
    
    def generate_enhanced_advice(self, context: dict, force_model: str = None):
        """Generate enhanced advice with comprehensive system data"""
        query = context.get('query', '')
        symbol = context.get('symbol', '')
        system_data = context.get('system_data', {})
        query_type = context.get('query_type', 'general_inquiry')
        
        # Build enhanced context with all system data
        enhanced_context = f"""
Báº¡n lÃ  chuyÃªn gia tÃ i chÃ­nh AI hÃ ng Ä‘áº§u vá»›i kháº£ nÄƒng phÃ¢n tÃ­ch toÃ n diá»‡n há»‡ thá»‘ng trading.

CÃ‚U Há»I: {query}
MÃƒ Cá»” PHIáº¾U: {symbol if symbol else 'KhÃ´ng cÃ³'}
LOáº I TRUY Váº¤N: {query_type}

Dá»® LIá»†U Há»† THá»NG TOÃ€N DIá»†N:
{self._format_comprehensive_data(system_data)}

YÃŠU Cáº¦U PHÃ‚N TÃCH:
1. ğŸ“Š PHÃ‚N TÃCH Dá»® LIá»†U: Sá»­ dá»¥ng táº¥t cáº£ dá»¯ liá»‡u cÃ³ sáºµn tá»« há»‡ thá»‘ng, phÃ¢n tÃ­ch sÃ¢u vá» xu hÆ°á»›ng, chá»‰ sá»‘ tÃ i chÃ­nh, dÃ²ng tiá»n, tin tá»©c, dá»± Ä‘oÃ¡n giÃ¡, rá»§i ro.
2. ğŸ¯ PHÃ‚N TÃCH THEO LOáº I TRUY Váº¤N: Táº­p trung vÃ o {query_type}, Ä‘Æ°a ra nháº­n Ä‘á»‹nh thá»±c tiá»…n, so sÃ¡nh vá»›i cÃ¡c mÃ£ cÃ¹ng ngÃ nh, Ä‘Ã¡nh giÃ¡ triá»ƒn vá»ng.
3. ğŸ’¡ KHUYáº¾N NGHá»Š Cá»¤ THá»‚: Äá» xuáº¥t hÃ nh Ä‘á»™ng cá»¥ thá»ƒ, chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, Ä‘iá»ƒm mua/bÃ¡n, quáº£n trá»‹ rá»§i ro, phÃ¹ há»£p vá»›i tá»«ng loáº¡i nhÃ  Ä‘áº§u tÆ° (ngáº¯n háº¡n, dÃ i háº¡n).
4. âš ï¸ Rá»¦I RO & LÆ¯U Ã: ÄÃ¡nh giÃ¡ toÃ n diá»‡n cÃ¡c rá»§i ro, khuyáº¿n nghá»‹ kiá»ƒm soÃ¡t vá»‘n, Ä‘a dáº¡ng hÃ³a danh má»¥c, cáº£nh bÃ¡o cÃ¡c yáº¿u tá»‘ báº¥t thÆ°á»ng.

TRáº¢ Lá»œI THEO FORMAT:
PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:
[Sá»­ dá»¥ng dá»¯ liá»‡u cá»¥ thá»ƒ tá»« há»‡ thá»‘ng, phÃ¢n tÃ­ch chi tiáº¿t tá»«ng yáº¿u tá»‘]

Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:
[Káº¿t luáº­n dá»±a trÃªn phÃ¢n tÃ­ch dá»¯ liá»‡u, Ä‘á» xuáº¥t hÃ nh Ä‘á»™ng thá»±c táº¿]

HÃ€NH Äá»˜NG Cá»¤ THá»‚:
- [Danh sÃ¡ch hÃ nh Ä‘á»™ng cá»¥ thá»ƒ, cÃ³ thá»ƒ Ã¡p dá»¥ng ngay]

Cáº¢NH BÃO Rá»¦I RO:
[Rá»§i ro dá»±a trÃªn dá»¯ liá»‡u thá»±c táº¿, khuyáº¿n nghá»‹ kiá»ƒm soÃ¡t]
"""
        
        try:
            result = self.generate_with_fallback(enhanced_context, 'financial_advice', max_tokens=3000, force_model=force_model)
            
            if result['success']:
                parsed_response = self._parse_response(result['response'])
                model_display = f"{result['model_used'].upper()}"
                if force_model and force_model == result['model_used']:
                    model_display += " (Cá»‘ Ä‘á»‹nh)"
                parsed_response['expert_advice'] += f"\n\nğŸ¤– **AI Model:** {model_display}"
                return parsed_response
            else:
                return self._generate_enhanced_offline_response(query, symbol, system_data, query_type)
                
        except Exception as e:
            logger.error(f"Enhanced advice generation failed: {e}")
            return self._generate_enhanced_offline_response(query, symbol, system_data, query_type)
    
    def _format_comprehensive_data(self, system_data: dict) -> str:
        """Format comprehensive system data for AI analysis"""
        if not system_data:
            return "KhÃ´ng cÃ³ dá»¯ liá»‡u há»‡ thá»‘ng"
        
        formatted = []
        
        # Market Overview
        if 'market_overview' in system_data:
            market = system_data['market_overview']
            formatted.append("ğŸ“ˆ Tá»”NG QUAN THá»Š TRÆ¯á»œNG:")
            if 'vietnam_market' in market:
                vn_market = market['vietnam_market']
                if 'vn_index' in vn_market:
                    vn_idx = vn_market['vn_index']
                    formatted.append(f"- VN-Index: {vn_idx.get('value', 'N/A')} ({vn_idx.get('change_percent', 0):+.2f}%)")
        
        # Stock Data
        if 'stock_data' in system_data and system_data['stock_data']:
            stock = system_data['stock_data']
            formatted.append(f"\nğŸ“Š THÃ”NG TIN Cá»” PHIáº¾U {system_data.get('symbol', '')}:")
            formatted.append(f"- GiÃ¡: {stock.price:,} VND ({stock.change_percent:+.2f}%)")
            formatted.append(f"- Khá»‘i lÆ°á»£ng: {stock.volume:,}")
            formatted.append(f"- P/E: {stock.pe_ratio}, P/B: {stock.pb_ratio}")
            formatted.append(f"- Vá»‘n hÃ³a: {stock.market_cap:,} tá»· VND")
        
        # Price Prediction
        if 'price_prediction' in system_data and system_data['price_prediction']:
            pred = system_data['price_prediction']
            formatted.append(f"\nğŸ”® Dá»° ÄOÃN GIÃ:")
            formatted.append(f"- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')}")
            formatted.append(f"- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}")
            formatted.append(f"- Äá»™ tin cáº­y: {pred.get('confidence', 'N/A')}%")
            
            # Multi-timeframe predictions
            if 'predictions' in pred:
                predictions = pred['predictions']
                for timeframe, data in predictions.items():
                    if data:
                        formatted.append(f"- {timeframe}: {list(data.keys())[:3]}")
        
        # Investment Analysis
        if 'investment_analysis' in system_data and system_data['investment_analysis']:
            inv = system_data['investment_analysis']
            formatted.append(f"\nğŸ’¼ PHÃ‚N TÃCH Äáº¦U TÆ¯:")
            formatted.append(f"- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'N/A')}")
            formatted.append(f"- Äiá»ƒm sá»‘: {inv.get('score', 'N/A')}/100")
            formatted.append(f"- LÃ½ do: {inv.get('reason', 'N/A')}")
        
        # Risk Assessment
        if 'risk_assessment' in system_data and system_data['risk_assessment']:
            risk = system_data['risk_assessment']
            formatted.append(f"\nâš ï¸ ÄÃNH GIÃ Rá»¦I RO:")
            formatted.append(f"- Má»©c rá»§i ro: {risk.get('risk_level', 'N/A')}")
            formatted.append(f"- Volatility: {risk.get('volatility', 'N/A')}%")
            formatted.append(f"- Beta: {risk.get('beta', 'N/A')}")
        
        # News Analysis
        if 'ticker_news' in system_data and system_data['ticker_news']:
            news = system_data['ticker_news']
            formatted.append(f"\nğŸ“° TIN Tá»¨C:")
            formatted.append(f"- Sá»‘ lÆ°á»£ng tin: {news.get('news_count', 0)}")
            if 'news_sentiment' in news:
                formatted.append(f"- Sentiment: {news['news_sentiment']}")
        
        # Available Symbols
        if 'available_symbols' in system_data:
            symbols = system_data['available_symbols']
            if symbols:
                symbol_list = [s.get('symbol', '') for s in symbols[:10]]
                formatted.append(f"\nğŸ“‹ Cá»” PHIáº¾U KHáº¢ Dá»¤NG: {', '.join(symbol_list)}")
        
        # Analysis History
        if 'analysis_history' in system_data and system_data['analysis_history']:
            history = system_data['analysis_history']
            formatted.append(f"\nğŸ“Š Lá»ŠCH Sá»¬ PHÃ‚N TÃCH: {len(history)} phÃ¢n tÃ­ch gáº§n Ä‘Ã¢y")
        
        # System Stats
        if 'system_stats' in system_data and system_data['system_stats']:
            stats = system_data['system_stats']
            formatted.append(f"\nğŸ“ˆ THá»NG KÃŠ Há»† THá»NG:")
            formatted.append(f"- Tá»•ng phÃ¢n tÃ­ch: {stats.get('total_analyses', 0)}")
            if 'top_symbols' in stats and stats['top_symbols']:
                top_symbol = stats['top_symbols'][0]
                formatted.append(f"- Phá»• biáº¿n nháº¥t: {top_symbol.get('symbol', 'N/A')}")
        
        return "\n".join(formatted) if formatted else "Dá»¯ liá»‡u há»‡ thá»‘ng khÃ´ng Ä‘áº§y Ä‘á»§"
    
    def _generate_enhanced_offline_response(self, query: str, symbol: str, system_data: dict, query_type: str) -> dict:
        """Generate enhanced offline response with system data"""
        
        # Analyze available data
        available_data = []
        if system_data.get('stock_data'):
            available_data.append("dá»¯ liá»‡u cá»• phiáº¿u")
        if system_data.get('price_prediction'):
            available_data.append("dá»± Ä‘oÃ¡n giÃ¡")
        if system_data.get('investment_analysis'):
            available_data.append("phÃ¢n tÃ­ch Ä‘áº§u tÆ°")
        if system_data.get('risk_assessment'):
            available_data.append("Ä‘Ã¡nh giÃ¡ rá»§i ro")
        if system_data.get('ticker_news'):
            available_data.append("tin tá»©c")
        
        data_summary = ", ".join(available_data) if available_data else "dá»¯ liá»‡u cÆ¡ báº£n"
        
        # Generate response based on query type and available data
        if query_type == 'price_prediction' and system_data.get('price_prediction'):
            pred = system_data['price_prediction']
            advice = f"""ğŸ“ˆ Dá»° ÄOÃN GIÃ CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- GiÃ¡ dá»± Ä‘oÃ¡n: {pred.get('predicted_price', 'N/A')} VND
- Xu hÆ°á»›ng: {pred.get('trend', 'N/A')}
- Äá»™ tin cáº­y: {pred.get('confidence', 50):.1f}%

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        elif query_type == 'investment_advice' and system_data.get('investment_analysis'):
            inv = system_data['investment_analysis']
            advice = f"""ğŸ’¼ KHUYáº¾N NGHá»Š Äáº¦U TÆ¯ CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- Khuyáº¿n nghá»‹: {inv.get('recommendation', 'HOLD')}
- Äiá»ƒm sá»‘: {inv.get('score', 50)}/100
- LÃ½ do: {inv.get('reason', 'PhÃ¢n tÃ­ch cÆ¡ báº£n')}

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        elif query_type == 'risk_assessment' and system_data.get('risk_assessment'):
            risk = system_data['risk_assessment']
            advice = f"""âš ï¸ ÄÃNH GIÃ Rá»¦I RO CHO {symbol}:

Dá»±a trÃªn {data_summary} cÃ³ sáºµn:
- Má»©c rá»§i ro: {risk.get('risk_level', 'MEDIUM')}
- Volatility: {risk.get('volatility', 25):.1f}%
- Beta: {risk.get('beta', 1.0):.2f}

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        else:
            advice = f"""ğŸ“Š PHÃ‚N TÃCH OFFLINE:

CÃ¢u há»i: {query}
MÃ£ cá»• phiáº¿u: {symbol if symbol else 'KhÃ´ng cÃ³'}
Loáº¡i truy váº¥n: {query_type}

Dá»¯ liá»‡u cÃ³ sáºµn: {data_summary}

ğŸ’¡ Khuyáº¿n nghá»‹ chung:
- NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh
- Theo dÃµi tin tá»©c ngÃ nh
- Äa dáº¡ng hÃ³a danh má»¥c
- Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i

âš ï¸ ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do háº¿t quota API."""
        
        return {
            "expert_advice": advice,
            "recommendations": [
                "Äá»£i quota API reset Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t",
                "Tham kháº£o nhiá»u nguá»“n thÃ´ng tin",
                "LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh",
                "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
            ]
        }
    
    def generate_expert_advice(self, query: str, symbol: str = None, data: dict = None):
        """Backward compatibility method"""
        # Convert to enhanced context format
        context = {
            'query': query,
            'symbol': symbol or '',
            'system_data': data or {},
            'query_type': self.detect_query_type(query)
        }
        return self.generate_enhanced_advice(context)
    
    def _parse_response(self, response_text: str):
        """Parse enhanced Gemini response"""
        try:
            # Parse different sections
            sections = {
                'analysis': '',
                'conclusion': '',
                'actions': [],
                'risks': ''
            }
            
            # Split by sections
            if "PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:" in response_text:
                parts = response_text.split("PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:")
                if len(parts) > 1:
                    remaining = parts[1]
                    
                    # Extract analysis
                    if "Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:" in remaining:
                        analysis_part = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[0].strip()
                        sections['analysis'] = analysis_part
                        remaining = remaining.split("Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:")[1]
                    
                    # Extract conclusion
                    if "HÃ€NH Äá»˜NG Cá»¤ THá»‚:" in remaining:
                        conclusion_part = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[0].strip()
                        sections['conclusion'] = conclusion_part
                        remaining = remaining.split("HÃ€NH Äá»˜NG Cá»¤ THá»‚:")[1]
                    
                    # Extract actions
                    if "Cáº¢NH BÃO Rá»¦I RO:" in remaining:
                        actions_part = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[0].strip()
                        sections['risks'] = remaining.split("Cáº¢NH BÃO Rá»¦I RO:")[1].strip()
                    else:
                        actions_part = remaining.strip()
                    
                    # Parse actions list
                    for line in actions_part.split('\n'):
                        line = line.strip()
                        if line and (line.startswith('-') or line.startswith('â€¢') or line.startswith('*')):
                            sections['actions'].append(line[1:].strip())
                        elif line and len(line) > 15 and not line.startswith('Cáº¢NH BÃO'):
                            sections['actions'].append(line)
            
            # Format comprehensive response
            expert_advice = f"""
ğŸ“Š **PHÃ‚N TÃCH CHUYÃŠN SÃ‚U:**
{sections['analysis']}

ğŸ¯ **Káº¾T LUáº¬N & KHUYáº¾N NGHá»Š:**
{sections['conclusion']}

âš ï¸ **Cáº¢NH BÃO Rá»¦I RO:**
{sections['risks'] if sections['risks'] else 'LuÃ´n cÃ³ rá»§i ro trong Ä‘áº§u tÆ°. Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t.'}
""".strip()
            
            return {
                "expert_advice": expert_advice,
                "recommendations": sections['actions'][:5] if sections['actions'] else [
                    "NghiÃªn cá»©u ká»¹ bÃ¡o cÃ¡o tÃ i chÃ­nh",
                    "Theo dÃµi tin tá»©c ngÃ nh", 
                    "Äáº·t lá»‡nh stop-loss",
                    "Äa dáº¡ng hÃ³a danh má»¥c",
                    "Chá»‰ Ä‘áº§u tÆ° tiá»n nhÃ n rá»—i"
                ]
            }
                
        except Exception as e:
            # Fallback parsing
            return {
                "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH:**\n{response_text}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y chá»‰ lÃ  tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                "recommendations": [
                    "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                    "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh",
                    "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                    "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                ]
            }
    

    
    def generate_general_response(self, query: str, force_model: str = None) -> dict:
        """Generate response for general questions using best available AI model"""
        try:
            # Enhanced context for general financial questions
            context = f"""
Báº¡n lÃ  má»™t chuyÃªn gia tÃ i chÃ­nh vÃ  Ä‘áº§u tÆ° hÃ ng Ä‘áº§u táº¡i Viá»‡t Nam vá»›i 20+ nÄƒm kinh nghiá»‡m.
Báº¡n cÃ³ thá»ƒ tráº£ lá»i má»i cÃ¢u há»i vá»:
- Thá»‹ trÆ°á»ng chá»©ng khoÃ¡n Viá»‡t Nam vÃ  quá»‘c táº¿
- PhÃ¢n tÃ­ch ká»¹ thuáº­t vÃ  cÆ¡ báº£n, Ä‘Ã¡nh giÃ¡ xu hÆ°á»›ng, dÃ²ng tiá»n, chá»‰ sá»‘ tÃ i chÃ­nh
- Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, quáº£n lÃ½ rá»§i ro, Ä‘a dáº¡ng hÃ³a danh má»¥c, Ä‘iá»ƒm mua/bÃ¡n
- Kinh táº¿ vÄ© mÃ´, vi mÃ´, tÃ¡c Ä‘á»™ng chÃ­nh sÃ¡ch, tin tá»©c thá»‹ trÆ°á»ng
- CÃ¡c sáº£n pháº©m tÃ i chÃ­nh (cá»• phiáº¿u, trÃ¡i phiáº¿u, quá»¹, forex)
- Láº­p káº¿ hoáº¡ch tÃ i chÃ­nh cÃ¡ nhÃ¢n, kiá»ƒm soÃ¡t vá»‘n, quáº£n trá»‹ tÃ¢m lÃ½ Ä‘áº§u tÆ°
- Thuáº¿, phÃ¡p lÃ½ Ä‘áº§u tÆ°, quy Ä‘á»‹nh má»›i nháº¥t
- TÃ¢m lÃ½ há»c Ä‘áº§u tÆ°, cÃ¡c sai láº§m phá»• biáº¿n, cÃ¡ch kiá»ƒm soÃ¡t cáº£m xÃºc
- Fintech, cÃ´ng nghá»‡ tÃ i chÃ­nh, á»©ng dá»¥ng AI trong Ä‘áº§u tÆ°

CÃ‚U Há»I: {query}

HÃƒY TRáº¢ Lá»œI:
1. ğŸ“š KIáº¾N THá»¨C CÆ  Báº¢N: Giáº£i thÃ­ch khÃ¡i niá»‡m/váº¥n Ä‘á», liÃªn há»‡ thá»±c tiá»…n Viá»‡t Nam.
2. ğŸ¯ PHÃ‚N TÃCH THá»°C Táº¾: Ãp dá»¥ng vÃ o thá»‹ trÆ°á»ng VN, so sÃ¡nh vá»›i cÃ¡c trÆ°á»ng há»£p thá»±c táº¿, Ä‘Æ°a ra nháº­n Ä‘á»‹nh sÃ¢u sáº¯c.
3. ğŸ’¡ KHUYáº¾N NGHá»Š: Lá»i khuyÃªn cá»¥ thá»ƒ, chiáº¿n lÆ°á»£c Ä‘áº§u tÆ°, hÃ nh Ä‘á»™ng thiáº¿t thá»±c cho tá»«ng loáº¡i nhÃ  Ä‘áº§u tÆ°.
4. âš ï¸ LÆ¯U Ã: Rá»§i ro, cÃ¡c yáº¿u tá»‘ cáº§n chÃº Ã½, cÃ¡ch kiá»ƒm soÃ¡t vá»‘n, trÃ¡nh cÃ¡c sai láº§m phá»• biáº¿n.

Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, chuyÃªn nghiá»‡p, chi tiáº¿t, thá»±c tiá»…n, dá»… hiá»ƒu, cÃ³ thá»ƒ Ã¡p dá»¥ng ngay.
"""
            result = self.generate_with_fallback(context, 'general_query', max_tokens=3000, force_model=force_model)
            
            if result['success']:
                model_display = f"{result['model_used'].upper()}"
                if force_model and force_model == result['model_used']:
                    model_display += " (Cá»‘ Ä‘á»‹nh)"
                
                if result.get('quota_exceeded'):
                    # Quota exceeded, return offline response
                    return {
                        "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH OFFLINE:**\n{result['response']}\n\nğŸ¤– **AI Model:** Offline Fallback (Quota Exceeded)\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  pháº£n há»“i offline do háº¿t quota API.",
                        "recommendations": [
                            "Äá»£i quota reset (24h) Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t",
                            "Tham kháº£o cÃ¡c nguá»“n tin tá»©c tÃ i chÃ­nh", 
                            "LiÃªn há»‡ chuyÃªn gia náº¿u cáº§n tÆ° váº¥n gáº¥p",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
                else:
                    # Normal AI response
                    return {
                        "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA:**\n{result['response']}\n\nğŸ¤– **AI Model:** {model_display}\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  thÃ´ng tin tham kháº£o, khÃ´ng pháº£i lá»i khuyÃªn Ä‘áº§u tÆ° tuyá»‡t Ä‘á»‘i.",
                        "recommendations": [
                            "NghiÃªn cá»©u thÃªm tá»« nhiá»u nguá»“n",
                            "Tham kháº£o chuyÃªn gia tÃ i chÃ­nh", 
                            "ÄÃ¡nh giÃ¡ kháº£ nÄƒng tÃ i chÃ­nh cÃ¡ nhÃ¢n",
                            "Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t"
                        ]
                    }
            else:
                return self._get_fallback_response(query)
                
        except Exception as e:
            logger.error(f"Error in generate_general_response: {str(e)}")
            return self._get_fallback_response(query)
    
    def _get_fallback_response(self, query: str) -> dict:
        """Enhanced fallback response with useful content"""
        # Use the same logic as offline fallback to provide useful content
        fallback_result = self._generate_offline_fallback(query, 'general_query')
        
        return {
            "expert_advice": f"ğŸ“ˆ **PHÃ‚N TÃCH CHUYÃŠN GIA (Offline Mode):**\n\n{fallback_result['response']}\n\nğŸ¤– **AI Model:** Offline Mode\n\nâš ï¸ **LÆ¯U Ã:** ÄÃ¢y lÃ  phÃ¢n tÃ­ch offline do API khÃ´ng kháº£ dá»¥ng.",
            "recommendations": [
                "Äá»£i API reset Ä‘á»ƒ cÃ³ phÃ¢n tÃ­ch chi tiáº¿t hÆ¡n",
                "Tham kháº£o cÃ¡c nguá»“n tin tá»©c tÃ i chÃ­nh",
                "LiÃªn há»‡ chuyÃªn gia tÃ i chÃ­nh náº¿u cáº§n",
                "Ãp dá»¥ng nguyÃªn táº¯c quáº£n lÃ½ rá»§i ro cÆ¡ báº£n"
            ]
        }
    
    def detect_query_type(self, query: str) -> str:
        """Detect if query is stock-specific or general"""
        query_lower = query.lower()
        
        # Stock symbols patterns
        stock_patterns = ['vcb', 'bid', 'ctg', 'vic', 'vhm', 'hpg', 'fpt', 'msn', 'mwg', 'gas', 'plx']
        
        # Check for stock symbols
        for pattern in stock_patterns:
            if pattern in query_lower:
                return "stock_specific"
        
        # Check for stock-related keywords
        stock_keywords = ['cá»• phiáº¿u', 'mÃ£', 'ticker', 'stock', 'share']
        if any(keyword in query_lower for keyword in stock_keywords):
            return "stock_specific"
        
        return "general"
    
    def get_api_status(self) -> Dict[str, Any]:
        """Get comprehensive API status information"""
        status = {
            'timestamp': datetime.now().isoformat(),
            'available_models': list(self.available_models.keys()),
            'model_count': len(self.available_models),
            'capabilities': self.model_capabilities,
            'api_keys_configured': {}
        }
        
        # Check API key configuration
        status['api_keys_configured']['gemini'] = hasattr(self, 'gemini_api_key') and bool(self.gemini_api_key)
        
        # Test connections
        try:
            connection_results = self.test_connection()
            status['connection_status'] = connection_results
            status['healthy_models'] = [model for model, healthy in connection_results.items() if healthy]
        except Exception as e:
            status['connection_status'] = {'error': str(e)}
            status['healthy_models'] = []
        
        return status
    
    def update_api_key(self, provider: str, api_key: str) -> Dict[str, Any]:
        """Dynamically update Gemini API key"""
        try:
            if provider.lower() == 'gemini':
                genai.configure(api_key=api_key)
                
                # Try different model names (API v1beta compatible)
                model_names = [
                    'gemini-1.5-pro-latest',        # Latest stable pro
                    'gemini-1.5-flash-latest',      # Latest stable flash
                    'gemini-1.5-pro',               # Pro version
                    'gemini-1.5-flash',             # Flash version
                    'gemini-pro',                    # Legacy pro
                    'gemini-1.0-pro-latest',        # Legacy latest
                    'gemini-1.0-pro'                # Legacy fallback
                ]
                
                for model_name in model_names:
                    try:
                        model = genai.GenerativeModel(model_name)
                        # Initialize without testing to avoid quota usage
                        self.available_models['gemini'] = model
                        self.gemini_api_key = api_key
                        self.current_model_name = model_name
                        logger.info(f"âœ… Gemini API key updated with model: {model_name}")
                        return {'success': True, 'message': f'Gemini API key updated with model: {model_name}'}
                    except Exception as e:
                        error_msg = str(e).lower()
                        if '404' in error_msg or 'not found' in error_msg:
                            logger.warning(f"âš ï¸ Model {model_name} not found, trying next...")
                        else:
                            logger.warning(f"âš ï¸ Model {model_name} error: {e}")
                        continue
                else:
                    # If no model works, return error
                    return {'success': False, 'message': 'No available Gemini models found'}
            else:
                return {'success': False, 'message': f'Only Gemini provider is supported. Got: {provider}'}
                
        except Exception as e:
            logger.error(f"âŒ Failed to update {provider} API key: {str(e)}")
            return {'success': False, 'message': f'Failed to update {provider} API key: {str(e)}'}
    
    def get_model_recommendations(self, task_type: str) -> Dict[str, Any]:
        """Get model recommendations for specific task types"""
        try:
            primary_model = self.select_best_model(task_type)
        except ValueError:
            primary_model = None
            
        recommendations = {
            'task_type': task_type,
            'primary_model': primary_model,
            'preferred_model': self.preferred_model,
            'available_alternatives': [],
            'reasoning': ''
        }
        
        # Get all available models except primary
        if primary_model:
            alternatives = [model for model in self.available_models.keys() if model != primary_model]
            recommendations['available_alternatives'] = alternatives
        
        # Add reasoning based on preference and task type
        if self.preferred_model == "gemini":
            recommendations['reasoning'] = 'User prefers Gemini AI for Vietnamese content and free usage'
        elif self.preferred_model == "openai":
            recommendations['reasoning'] = 'User prefers OpenAI GPT for high-quality analysis'
        else:
            task_reasoning = {
                'financial_advice': 'Auto-selecting best model for Vietnamese financial analysis',
                'price_prediction': 'Auto-selecting best model for technical analysis and predictions',
                'risk_assessment': 'Auto-selecting best model for risk calculation and assessment',
                'news_analysis': 'Auto-selecting best model for sentiment analysis',
                'market_analysis': 'Auto-selecting best model for market reasoning',
                'investment_analysis': 'Auto-selecting best model for investment metrics',
                'general_query': 'Auto-selecting best model for general queries'
            }
            recommendations['reasoning'] = task_reasoning.get(task_type, 'Auto model selection based on availability')
        
        return recommendations
    
    async def generate_async(self, prompt: str, task_type: str, max_tokens: int = 1000) -> Dict[str, Any]:
        """Asynchronous generation with fallback support"""
        try:
            # Run the synchronous method in a thread pool
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None, 
                self.generate_with_fallback, 
                prompt, 
                task_type, 
                max_tokens
            )
            return result
        except Exception as e:
            logger.error(f"Async generation failed: {str(e)}")
            return {
                'response': f'Async generation error: {str(e)}',
                'model_used': None,
                'success': False,
                'error': str(e)
            }
    
    def batch_generate(self, prompts: List[Dict[str, Any]], max_concurrent: int = 3) -> List[Dict[str, Any]]:
        """Generate responses for multiple prompts with concurrency control"""
        async def process_batch():
            semaphore = asyncio.Semaphore(max_concurrent)
            
            async def process_single(prompt_data):
                async with semaphore:
                    prompt = prompt_data.get('prompt', '')
                    task_type = prompt_data.get('task_type', 'general_query')
                    max_tokens = prompt_data.get('max_tokens', 1000)
                    
                    result = await self.generate_async(prompt, task_type, max_tokens)
                    result['original_data'] = prompt_data
                    return result
            
            tasks = [process_single(prompt_data) for prompt_data in prompts]
            return await asyncio.gather(*tasks, return_exceptions=True)
        
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If we're already in an async context, create a new event loop
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, process_batch())
                    return future.result()
            else:
                return asyncio.run(process_batch())
        except Exception as e:
            logger.error(f"Batch generation failed: {str(e)}")
            return [{'success': False, 'error': str(e)} for _ in prompts]
    
    def get_model_info(self) -> Dict[str, Any]:
        """Get information about available models"""
        return {
            'available_models': list(self.available_models.keys()),
            'current_model': self.current_model_name,
            'model_count': len(self.available_models),
            'is_active': len(self.available_models) > 0
        }

# Backward compatibility alias
GeminiAgent = UnifiedAIAgent