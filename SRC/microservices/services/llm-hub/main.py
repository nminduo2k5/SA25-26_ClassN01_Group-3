from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import redis
import json
import os
import google.generativeai as genai
from typing import Optional, Dict, Any
import asyncio

app = FastAPI(title="LLM Hub Service", version="1.0.0")

# Redis connection
redis_client = redis.Redis(host='redis', port=6379, decode_responses=True)

# Configure Gemini
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)

class LLMRequest(BaseModel):
    prompt: str
    model: str = "gemini"
    temperature: float = 0.7
    max_tokens: int = 1000
    cache_key: Optional[str] = None

class LLMResponse(BaseModel):
    response: str
    model_used: str
    cached: bool = False
    tokens_used: Optional[int] = None

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "service": "llm-hub",
        "models_available": ["gemini", "openai", "offline"],
        "redis_connected": redis_client.ping()
    }

@app.get("/models")
async def get_available_models():
    models = {
        "gemini": {
            "available": bool(GEMINI_API_KEY),
            "models": ["gemini-2.0-flash-exp", "gemini-1.5-pro"]
        },
        "openai": {
            "available": bool(os.getenv('OPENAI_API_KEY')),
            "models": ["gpt-4", "gpt-3.5-turbo"]
        },
        "offline": {
            "available": True,
            "models": ["fallback"]
        }
    }
    return models

@app.post("/generate", response_model=LLMResponse)
async def generate_response(request: LLMRequest):
    # Check cache first
    if request.cache_key:
        cached_response = redis_client.get(f"llm_cache:{request.cache_key}")
        if cached_response:
            cached_data = json.loads(cached_response)
            return LLMResponse(**cached_data, cached=True)
    
    try:
        if request.model == "gemini" and GEMINI_API_KEY:
            response = await generate_gemini_response(request)
        elif request.model == "openai" and os.getenv('OPENAI_API_KEY'):
            response = await generate_openai_response(request)
        else:
            response = generate_offline_response(request)
        
        # Cache the response
        if request.cache_key:
            cache_data = response.dict()
            cache_data['cached'] = False
            redis_client.setex(
                f"llm_cache:{request.cache_key}", 
                3600,  # 1 hour
                json.dumps(cache_data)
            )
        
        return response
        
    except Exception as e:
        # Fallback to offline mode
        return generate_offline_response(request)

async def generate_gemini_response(request: LLMRequest) -> LLMResponse:
    try:
        model = genai.GenerativeModel('gemini-2.0-flash-exp')
        response = model.generate_content(
            request.prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=request.temperature,
                max_output_tokens=request.max_tokens,
            )
        )
        
        return LLMResponse(
            response=response.text,
            model_used="gemini-2.0-flash-exp",
            tokens_used=response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else None
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Gemini API error: {str(e)}")

async def generate_openai_response(request: LLMRequest) -> LLMResponse:
    # OpenAI implementation would go here
    raise HTTPException(status_code=501, detail="OpenAI integration not implemented yet")

def generate_offline_response(request: LLMRequest) -> LLMResponse:
    """Fallback offline responses for common stock analysis queries"""
    
    offline_responses = {
        "analysis": """ğŸ“ˆ OFFLINE ANALYSIS:
        
Há»‡ thá»‘ng Ä‘ang hoáº¡t Ä‘á»™ng á»Ÿ cháº¿ Ä‘á»™ offline. DÆ°á»›i Ä‘Ã¢y lÃ  phÃ¢n tÃ­ch cÆ¡ báº£n:

ğŸ’¡ NguyÃªn táº¯c Ä‘áº§u tÆ° cÆ¡ báº£n:
- P/E < 15 thÆ°á»ng Ä‘Æ°á»£c coi lÃ  háº¥p dáº«n
- Äa dáº¡ng hÃ³a danh má»¥c Ä‘á»ƒ giáº£m rá»§i ro  
- Chá»‰ Ä‘áº§u tÆ° sá»‘ tiá»n cÃ³ thá»ƒ cháº¥p nháº­n máº¥t

âš ï¸ LÆ°u Ã½: ÄÃ¢y lÃ  phÃ¢n tÃ­ch cÆ¡ báº£n. Vui lÃ²ng kiá»ƒm tra API key hoáº·c thá»­ láº¡i sau.""",
        
        "prediction": """ğŸ”® Dá»° ÄOÃN GIÃ (OFFLINE):
        
KhÃ´ng thá»ƒ thá»±c hiá»‡n dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c trong cháº¿ Ä‘á»™ offline.

ğŸ“Š Khuyáº¿n nghá»‹ chung:
- Theo dÃµi xu hÆ°á»›ng thá»‹ trÆ°á»ng tá»•ng thá»ƒ
- PhÃ¢n tÃ­ch ká»¹ thuáº­t cÆ¡ báº£n (MA, RSI, MACD)
- Xem xÃ©t cÃ¡c yáº¿u tá»‘ vÄ© mÃ´

â° API quota thÆ°á»ng reset sau 24 giá».""",
        
        "risk": """âš ï¸ ÄÃNH GIÃ Rá»¦I RO (OFFLINE):
        
NguyÃªn táº¯c quáº£n lÃ½ rá»§i ro cÆ¡ báº£n:
- KhÃ´ng Ä‘áº§u tÆ° quÃ¡ 5% tá»•ng tÃ i sáº£n vÃ o 1 cá»• phiáº¿u
- Äáº·t stop-loss á»Ÿ má»©c 10-15%
- Äa dáº¡ng hÃ³a theo ngÃ nh nghá»

ğŸ“ˆ Chá»‰ sá»‘ rá»§i ro cáº§n theo dÃµi:
- Beta (Ä‘á»™ biáº¿n Ä‘á»™ng so vá»›i thá»‹ trÆ°á»ng)
- VaR (Value at Risk)
- Sharpe Ratio"""
    }
    
    # Simple keyword matching for offline responses
    prompt_lower = request.prompt.lower()
    
    if any(word in prompt_lower for word in ['dá»± Ä‘oÃ¡n', 'predict', 'giÃ¡', 'price']):
        response_text = offline_responses['prediction']
    elif any(word in prompt_lower for word in ['rá»§i ro', 'risk', 'an toÃ n', 'safety']):
        response_text = offline_responses['risk']
    else:
        response_text = offline_responses['analysis']
    
    return LLMResponse(
        response=response_text,
        model_used="offline-fallback"
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8010)