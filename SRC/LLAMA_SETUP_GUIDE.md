# H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t Llama Local

## üéØ T·ªïng quan
Llama ch·∫°y local qua Ollama - kh√¥ng c·∫ßn API key, ho√†n to√†n mi·ªÖn ph√≠.

## üì• C√†i ƒë·∫∑t Ollama

### Windows:
1. Download t·ª´: https://ollama.ai
2. Ch·∫°y file installer
3. M·ªü Command Prompt/PowerShell

### macOS:
```bash
brew install ollama
```

### Linux:
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

## üöÄ C√†i ƒë·∫∑t Models

```bash
# Model ch√≠nh (khuy·∫øn ngh·ªã)
ollama pull llama3.1:8b

# Models kh√°c (t√πy ch·ªçn)
ollama pull llama3:8b
ollama pull llama2:7b
ollama pull codellama:7b
```

## ‚ñ∂Ô∏è Ch·∫°y Ollama

```bash
# Kh·ªüi ƒë·ªông server (c·∫ßn ch·∫°y tr∆∞·ªõc khi d√πng)
ollama serve

# Ho·∫∑c test tr·ª±c ti·∫øp
ollama run llama3.1:8b
```

## üîß Ki·ªÉm tra

```bash
# Xem models ƒë√£ c√†i
ollama list

# Test API
curl http://localhost:11434/api/generate -d '{
  "model": "llama3.1:8b",
  "prompt": "Hello"
}'
```

## üêç Test trong Python

```python
from llm.llama_llm import LlamaLLM

# T·∫°o instance
llama = LlamaLLM()
print(f"Available: {llama.is_available}")

# Test generation
if llama.is_available:
    result = llama.generate("Xin ch√†o")
    print(result['response'])
```

## ‚ö†Ô∏è Troubleshooting

### L·ªói "connection refused":
- Ch·∫°y `ollama serve` tr∆∞·ªõc
- Ki·ªÉm tra port 11434 c√≥ b·ªã block kh√¥ng

### L·ªói "model not found":
- Ch·∫°y `ollama pull llama3.1:8b`
- Ki·ªÉm tra `ollama list`

### Performance ch·∫≠m:
- D√πng model nh·ªè h∆°n: `llama2:7b`
- TƒÉng RAM/CPU
- Gi·∫£m max_tokens

## üí° Tips

- **RAM c·∫ßn**: T·ªëi thi·ªÉu 8GB cho model 7B
- **T·ªëc ƒë·ªô**: Local n√™n c√≥ th·ªÉ ch·∫≠m h∆°n cloud API
- **Offline**: Ho·∫°t ƒë·ªông ho√†n to√†n offline
- **Mi·ªÖn ph√≠**: Kh√¥ng gi·ªõi h·∫°n usage